{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b2737c-1a35-4387-9b87-7284ede87948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda72b9a-97f3-4c84-a0d4-276a15896831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append(\"../../\")\n",
    "#from Config import *\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d478b0cb-8e90-4526-a57d-8631ea1a8479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef68cac-d85f-4c1b-a295-25672d9a20e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cohort 1 Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ffc1164-7bee-4089-ac7e-081318f5dca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dummy data\n",
    "\n",
    "cohort_1_path=\"../Dummy_Data/Cohort_1_synth.xlsx\"\n",
    "cohort_1_df = pd.read_excel(cohort_1_path)\n",
    "\n",
    "cohort_1_df = spark.createDataFrame(cohort_1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b58f407-123a-43d4-9067-43d9d8a139f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# real data\n",
    "\n",
    "# cohort_1_df = spark.read.format(\"parquet\").load(cohort_1_link)\n",
    "#display(cohort_1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd2eab6-5bcc-4e78-8e95-cbadb7442c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column to categorize ndl_wait_band\n",
    "cohort_1_df = cohort_1_df.withColumn(\"wait_band_category\", when(col(\"ndl_wait_band\") == \"<= 18 weeks\", \"Less than 18 weeks\").otherwise(\"More than 18 weeks\"))\n",
    "\n",
    "# Group by Specialty and wait_band_category to get the count\n",
    "wait_band_counts_specialty = cohort_1_df.groupBy(\"Specialty\", \"wait_band_category\").count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "wait_band_counts_specialty_pd = wait_band_counts_specialty.select(\"Specialty\", \"wait_band_category\", \"count\").toPandas()\n",
    "\n",
    "# Order the bars from largest to smallest\n",
    "wait_band_counts_specialty_pd = wait_band_counts_specialty_pd.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "# Plot using seaborn for Specialty\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=wait_band_counts_specialty_pd, x=\"count\", y=\"Specialty\", hue=\"wait_band_category\", order=wait_band_counts_specialty_pd[\"Specialty\"].unique())\n",
    "plt.ylabel(\"Specialty\", fontsize=18)\n",
    "plt.xlabel(\"Count\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(title=\"Wait Band Category\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Create a new column to categorize provider and provider_site\n",
    "cohort_1_df = cohort_1_df.withColumn(\"provider_category\", when(col(\"wlmds_provider\").contains(\"RR8\"), \"Contains RR8\").otherwise(\"Does not contain RR8\"))\n",
    "cohort_1_df = cohort_1_df.withColumn(\"provider_site_category\", when(col(\"wlmds_provider_site\").contains(\"RR8\"), \"Contains RR8\").otherwise(\"Does not contain RR8\"))\n",
    "\n",
    "# Group by wlmds_provider and wait_band_category to get the count and proportion\n",
    "wait_band_counts_provider = cohort_1_df.groupBy(\"provider_category\", \"wait_band_category\").count()\n",
    "total_counts_provider = wait_band_counts_provider.groupBy(\"provider_category\").agg(sum(\"count\").alias(\"total_count\"))\n",
    "wait_band_counts_provider = wait_band_counts_provider.join(total_counts_provider, \"provider_category\").withColumn(\"proportion\", col(\"count\") / col(\"total_count\"))\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "wait_band_counts_provider_pd = wait_band_counts_provider.select(\"provider_category\", \"wait_band_category\", \"proportion\").toPandas()\n",
    "\n",
    "# Order the bars from largest to smallest\n",
    "wait_band_counts_provider_pd = wait_band_counts_provider_pd.sort_values(by=\"proportion\", ascending=False)\n",
    "\n",
    "# Plot using seaborn for wlmds_provider\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=wait_band_counts_provider_pd, y=\"proportion\", x=\"provider_category\", hue=\"wait_band_category\", order=wait_band_counts_provider_pd[\"provider_category\"].unique())\n",
    "plt.xlabel(\"Provider\", fontsize=18)\n",
    "plt.ylabel(\"Proportion\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(title=\"Wait Band Category\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Group by wlmds_provider_site and wait_band_category to get the count and proportion\n",
    "wait_band_counts_provider_site = cohort_1_df.groupBy(\"provider_site_category\", \"wait_band_category\").count()\n",
    "total_counts_provider_site = wait_band_counts_provider_site.groupBy(\"provider_site_category\").agg(sum(\"count\").alias(\"total_count\"))\n",
    "wait_band_counts_provider_site = wait_band_counts_provider_site.join(total_counts_provider_site, \"provider_site_category\").withColumn(\"proportion\", col(\"count\") / col(\"total_count\"))\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "wait_band_counts_provider_site_pd = wait_band_counts_provider_site.select(\"provider_site_category\", \"wait_band_category\", \"proportion\").toPandas()\n",
    "\n",
    "# Order the bars from largest to smallest\n",
    "wait_band_counts_provider_site_pd = wait_band_counts_provider_site_pd.sort_values(by=\"proportion\", ascending=False)\n",
    "\n",
    "# Plot using seaborn for wlmds_provider_site\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=wait_band_counts_provider_site_pd, y=\"proportion\", x=\"provider_site_category\", hue=\"wait_band_category\", order=wait_band_counts_provider_site_pd[\"provider_site_category\"].unique())\n",
    "plt.xlabel(\"Provider Site\", fontsize=18)\n",
    "plt.ylabel(\"Proportion\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(title=\"Wait Band Category\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dcdb883-5a8e-488c-9ca4-5466fc7857f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gynaecology_df = cohort_1_df.filter(cohort_1_df.Specialty == \"Gynaecology\")\n",
    "\n",
    "median_wait_length = gynaecology_df.groupBy(\"ndl_ltc\").agg(median(\"ndl_wait_length\").alias(\"median_wait_length\"))\n",
    "\n",
    "display(median_wait_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de990f74-767e-4467-a9de-1f16ad96e6c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the median \"ndl_wait_length\" for each value of \"ndl_start_date_count\" for each \"ndl_ltc\"\n",
    "median_wait_length_start_date_count = cohort_1_df.groupBy(\"ndl_ltc\", \"ndl_start_date_count\").agg(median(\"ndl_wait_length\").alias(\"median_wait_length\"))\n",
    "\n",
    "median_wait_length_start_bucket_count = cohort_1_df.groupBy(\"ndl_ltc\", \"ndl_start_bucket\").agg(median(\"ndl_wait_length\").alias(\"median_wait_length\"))\n",
    "\n",
    "display(median_wait_length_start_date_count)\n",
    "display(median_wait_length_start_bucket_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9616ed37-a3b5-4d20-95bb-124b81851af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import countDistinct, col\n",
    "\n",
    "# Filter records with more than 1 distinct \"wlmds_rtt_start_date_conc3\" for each \"wlmds_patient_id\" and \"wlmds_treatment_function_code\"\n",
    "distinct_rtt_start_date_count = cohort_1_df.groupBy(\"wlmds_patient_id\", \"wlmds_treatment_function_code\", \"Specialty\") \\\n",
    "    .agg(countDistinct(\"wlmds_rtt_start_date_conc3\").alias(\"distinct_rtt_start_date_count\")) \\\n",
    "    .filter(\"distinct_rtt_start_date_count > 1\")\n",
    "\n",
    "# Filter records where \"wlmds_type_changes_last\" contains \"IRTT\"\n",
    "irtt_records = cohort_1_df.filter(col(\"wlmds_type_changes_last\").contains(\"IRTT\"))\n",
    "\n",
    "# Count the number of such records for each \"Specialty\"\n",
    "specialty_record_count = distinct_rtt_start_date_count.groupBy(\"Specialty\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"record_count\")\n",
    "\n",
    "# Count the number of IRTT records for each \"Specialty\"\n",
    "irtt_specialty_record_count = irtt_records.groupBy(\"Specialty\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"irtt_record_count\")\n",
    "\n",
    "# Join the two counts\n",
    "specialty_record_count = specialty_record_count.join(irtt_specialty_record_count, on=\"Specialty\", how=\"left\").fillna(0)\n",
    "\n",
    "# Calculate the proportion of IRTT records\n",
    "specialty_record_count = specialty_record_count.withColumn(\"irtt_proportion\", col(\"irtt_record_count\") / col(\"record_count\"))\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "specialty_record_count_pd = specialty_record_count.orderBy(\"record_count\", ascending=False).toPandas()\n",
    "\n",
    "# Plot the count of records for each \"Specialty\" and highlight the proportion of IRTT records\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=specialty_record_count_pd, x='Specialty', y='record_count', order=specialty_record_count_pd['Specialty'])\n",
    "plt.xlabel('Specialty')\n",
    "plt.ylabel('Record Count')\n",
    "plt.title('Count of Records where Patients have >1 Distinct RTT Start Date for Each Specialty')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add a secondary y-axis for the proportion of IRTT records\n",
    "ax2 = plt.twinx()\n",
    "sns.lineplot(data=specialty_record_count_pd, x='Specialty', y='irtt_proportion', color='red', marker='o', ax=ax2)\n",
    "ax2.set_ylabel('Proportion of Records with an Admission')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6de8463c-4473-4301-a93b-5c17594fafa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, col\n",
    "\n",
    "# Filter records with more than 1 distinct \"wlmds_rtt_start_date_conc3\" for each \"wlmds_patient_id\" and \"wlmds_treatment_function_code\"\n",
    "distinct_rtt_start_date_count = cohort_1_df.groupBy(\"wlmds_patient_id\", \"wlmds_treatment_function_code\") \\\n",
    "    .agg(countDistinct(\"wlmds_rtt_start_date_conc3\").alias(\"distinct_rtt_start_date_count\")) \\\n",
    "    .filter(\"distinct_rtt_start_date_count > 1\")\n",
    "\n",
    "# Filter records where \"Specialty\" = \"Other - Medical Services\"\n",
    "filtered_records = cohort_1_df.filter(col(\"Specialty\") == \"Other - Medical Services\")\n",
    "\n",
    "# Join the filtered records with the distinct_rtt_start_date_count\n",
    "joined_records = filtered_records.join(distinct_rtt_start_date_count, on=[\"wlmds_patient_id\", \"wlmds_treatment_function_code\"], how=\"inner\")\n",
    "\n",
    "# Count the frequency of groups for each \"wlmds_treatment_function_code\"\n",
    "group_frequency = joined_records.groupBy(\"wlmds_treatment_function_code\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"group_frequency\")\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "group_frequency_pd = group_frequency.orderBy(\"group_frequency\", ascending=False).toPandas()\n",
    "\n",
    "# Plot the frequency of groups for each \"wlmds_treatment_function_code\"\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=group_frequency_pd, x='wlmds_treatment_function_code', y='group_frequency', order=group_frequency_pd['wlmds_treatment_function_code'])\n",
    "plt.xlabel('Treatment Function Code')\n",
    "plt.ylabel('Group Frequency')\n",
    "plt.title('Frequency of Groups with >1 Distinct RTT Start Date for Each Treatment Function Code (Specialty: Other - Medical Services)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef76710-2b0d-4298-8964-6f08020e7808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the order for the x-axis\n",
    "order = [\"No LTCs\", \"Single LTC\", \"Comorbidities\", \"Multimorbidities\"]\n",
    "\n",
    "median_wait_length_start_date_count_pd = median_wait_length_start_date_count.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=median_wait_length_start_date_count_pd, x='ndl_ltc', y='median_wait_length', hue='ndl_start_date_count', order=order)\n",
    "plt.xlabel('LTC Bracket')\n",
    "plt.ylabel('Wait Length (Days)')\n",
    "plt.title('Median Waiting Time Across LTC Bracket and Start Date Count (Gynaecology)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"NDL Start Date Count\")\n",
    "plt.tight_layout()\n",
    "\n",
    "average_median_wait_length = median_wait_length_start_date_count_pd.groupby('ndl_ltc')['median_wait_length'].mean().reset_index()\n",
    "\n",
    "# Ensure the order for the line plot\n",
    "sns.lineplot(data=average_median_wait_length, x='ndl_ltc', y='median_wait_length', color='red', marker='o', label='Average Median Wait Length').set_xticks(order)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71dc7b4-4d5d-4a2d-bd5b-3c0703117831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the columns for which to create box and whisker plots, excluding \"Specialty\" and \"ndl_age\"\n",
    "columns = [\"ndl_ethnicity\", \"ndl_ltc\", \"ndl_imd_quantile\", \"Frailty_level\"]\n",
    "\n",
    "# Create a grid for the plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(columns):\n",
    "    # Select the relevant columns for plotting\n",
    "    plot_df = cohort_1_df.select(column, \"ndl_wait_length\").toPandas()\n",
    "\n",
    "    # Sort values by median_wait_length in descending order\n",
    "    order = plot_df.groupby(column)[\"ndl_wait_length\"].median().sort_values(ascending=False).index\n",
    "\n",
    "    sns.boxplot(ax=axes[i], data=plot_df, x=column, y='ndl_wait_length', order=order)\n",
    "    axes[i].set_xlabel(column.replace('_', ' ').title(), fontsize=18)\n",
    "    axes[i].set_ylabel('Wait Length (Days)', fontsize=18)\n",
    "    axes[i].tick_params(axis='x', rotation=45, labelsize=16)\n",
    "    axes[i].tick_params(axis='y', labelsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "890c2b72-9382-4072-8c38-0275542b1054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, median, desc\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter out null values in \"ndl_age_band\"\n",
    "df_filtered = cohort_1_df.filter(col(\"ndl_age_band\").isNotNull())\n",
    "\n",
    "# Calculate the median \"ndl_wait_length\" for each value of \"ndl_age_band\"\n",
    "median_wait_length_age_band = df_filtered.groupBy(\"ndl_age_band\").agg(median(\"ndl_wait_length\").alias(\"median_wait_length\")).orderBy(desc(\"median_wait_length\"))\n",
    "\n",
    "# Calculate the median \"ndl_wait_length\" for each value of \"Specialty\"\n",
    "median_wait_length_specialty = cohort_1_df.groupBy(\"Specialty\").agg(median(\"ndl_wait_length\").alias(\"median_wait_length\")).orderBy(desc(\"median_wait_length\"))\n",
    "\n",
    "# Convert to Pandas DataFrames for plotting\n",
    "median_wait_length_age_band_pd = median_wait_length_age_band.toPandas()\n",
    "median_wait_length_specialty_pd = median_wait_length_specialty.toPandas()\n",
    "\n",
    "# Define the order for ndl_age_band\n",
    "age_band_order = [\"<= 10\", \"11-17\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65-74\", \"75-84\", \"84+\"]\n",
    "\n",
    "# Plot bar charts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(24, 10))\n",
    "\n",
    "# Plot for \"ndl_age_band\"\n",
    "sns.barplot(ax=axes[0], data=median_wait_length_age_band_pd, y='ndl_age_band', x='median_wait_length', order=age_band_order)\n",
    "axes[0].set_ylabel('NDL Age Band', fontsize=18)\n",
    "axes[0].set_xlabel('Median Wait Length (Days)', fontsize=18)\n",
    "axes[0].tick_params(axis='y', rotation=0, labelsize=18)\n",
    "axes[0].tick_params(axis='x', labelsize=18)\n",
    "for bar in axes[0].patches:\n",
    "    if bar.get_width() > 100:\n",
    "        bar.set_color('red')\n",
    "\n",
    "# Plot for \"Specialty\"\n",
    "sns.barplot(ax=axes[1], data=median_wait_length_specialty_pd, y='Specialty', x='median_wait_length', order=median_wait_length_specialty_pd['Specialty'])\n",
    "axes[1].set_ylabel('Specialty', fontsize=18)\n",
    "axes[1].set_xlabel('Median Wait Length (Days)', fontsize=18)\n",
    "axes[1].tick_params(axis='y', rotation=0, labelsize=18)\n",
    "axes[1].tick_params(axis='x', labelsize=18)\n",
    "for bar in axes[1].patches:\n",
    "    if bar.get_width() > 100:\n",
    "        bar.set_color('red')\n",
    "\n",
    "# Add legend\n",
    "fig.legend(['Median waits over 100 days are highlighted'], loc='upper right', bbox_to_anchor=(0.99, 0.15), fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b607fe-9f9f-4d3e-84ef-7c1aee397511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import median\n",
    "\n",
    "gynaecology_df = cohort_1_df.filter(cohort_1_df.Specialty == \"Gynaecology\")\n",
    "\n",
    "median_wait_length = gynaecology_df.groupBy(\"ndl_ethnicity\").agg(median(\"ndl_wait_length\").alias(\"median_wait_length\"))\n",
    "\n",
    "display(median_wait_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cea6b72-2b72-452a-85ad-8b368a9c289f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract waiting time column\n",
    "waiting_times = cohort_1_df.select(\"ndl_wait_length\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(waiting_times, bins=30, edgecolor='black')\n",
    "plt.title('Histogram of Waiting Times')\n",
    "plt.xlabel('Waiting Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb023a17-1279-4d94-8063-e62966a4ee44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the calculate_wait_band_percentages function\n",
    "def calculate_wait_band_percentages(df, wait_band_col, speciality_col):\n",
    "    total_counts = df.groupBy(speciality_col).count().withColumnRenamed(\"count\", \"total_count\")\n",
    "    wait_band_counts = df.groupBy(speciality_col, wait_band_col).count().withColumnRenamed(\"count\", \"wait_band_count\")\n",
    "    return wait_band_counts.join(total_counts, on=speciality_col) \\\n",
    "                           .withColumn(\"percentage\", (F.col(\"wait_band_count\") / F.col(\"total_count\")) * 100)\n",
    "\n",
    "# Calculate wait band percentages for specialties\n",
    "df_speciality_wait_band = calculate_wait_band_percentages(cohort_1_df, \"ndl_wait_band\", \"Specialty\")\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "pdf_speciality_wait_band = df_speciality_wait_band.select(\"Specialty\", \"ndl_wait_band\", \"percentage\").toPandas()\n",
    "\n",
    "# Pivot the DataFrame for stacked bar plot\n",
    "pivot_df = pdf_speciality_wait_band.pivot(index='Specialty', columns='ndl_wait_band', values='percentage').fillna(0)\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "pivot_df.plot(kind='bar', stacked=True, figsize=(10, 7))\n",
    "\n",
    "plt.title('Wait Band Percentages as Stacked Bars for Speciality')\n",
    "plt.xlabel('Speciality')\n",
    "plt.ylabel('Percentage')\n",
    "plt.legend(title='Wait Band', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584b02dc-5e5c-4884-b243-c86ae0dc6434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Group by patient ID and count distinct TFCs for each patient\n",
    "patient_tfc_counts = cohort_1_df.groupBy(\"wlmds_patient_id\").agg(\n",
    "    countDistinct(\"tfc\").alias(\"distinct_tfc_count\")\n",
    ")\n",
    "\n",
    "# Filter patients with more than one distinct TFC\n",
    "patients_with_multiple_tfc = patient_tfc_counts.filter(col(\"distinct_tfc_count\") > 1)\n",
    "\n",
    "# Count the number of patients with multiple TFCs\n",
    "num_patients_with_multiple_tfc = patients_with_multiple_tfc.count()\n",
    "\n",
    "num_patients_with_multiple_tfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84708ba0-2e04-4b37-a1f5-3f60937600f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for plotting\n",
    "df_pandas = cohort_1_df.select(\"ndl_wait_length\", \"Sex\").toPandas()\n",
    "\n",
    "# Create the violin plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(hue=\"Sex\", y=\"ndl_wait_length\", data=df_pandas, split=True, palette=\"Set2\", inner=\"quart\", gap=0)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title(\"Violin Plot of Waiting Length by Sex\")\n",
    "plt.xlabel(\"Sex\")\n",
    "plt.ylabel(\"Waiting Length (Days)\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ffced07c-5f77-4384-946c-42e1438231e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Plotting ethnicity proportions for each specialty and wait band\n",
    "df_ethnicity_specialty_band = cohort_1_df.groupBy(\"Specialty\", \"ndl_ethnicity\", \"ndl_wait_band\").count()\n",
    "df_total_specialty_band = cohort_1_df.groupBy(\"Specialty\", \"ndl_wait_band\").count().withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "df_proportion_band = df_ethnicity_specialty_band.join(df_total_specialty_band, on=[\"Specialty\", \"ndl_wait_band\"])\n",
    "df_proportion_band = df_proportion_band.withColumn(\"proportion\", col(\"count\") / col(\"total_count\"))\n",
    "\n",
    "df_proportion_band_pandas = df_proportion_band.select(\"Specialty\", \"ndl_ethnicity\", \"ndl_wait_band\", \"proportion\").toPandas()\n",
    "\n",
    "ethnicity_baseline_mapping = {\n",
    "    \"asian_background\": 0.097,\n",
    "    \"black_background\": 0.056,\n",
    "    \"white_background\": 0.79,\n",
    "    \"mixed_background\": 0.034,\n",
    "    \"other_background\": 0.023\n",
    "}\n",
    "\n",
    "wait_band_order = [\"<= 18 weeks\", \"> 18 weeks\", \"> 36 weeks\", \"> 52 weeks\"]\n",
    "\n",
    "specialties = df_proportion_band_pandas[\"Specialty\"].unique()\n",
    "for specialty in specialties:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    data = df_proportion_band_pandas[df_proportion_band_pandas[\"Specialty\"] == specialty]\n",
    "    data = data.sort_values(by=\"proportion\", ascending=False)\n",
    "    sns.barplot(x=\"ndl_ethnicity\", y=\"proportion\", hue=\"ndl_wait_band\", data=data, palette=\"Set2\", hue_order=wait_band_order)\n",
    "    \n",
    "    sorted_baseline = dict(sorted(ethnicity_baseline_mapping.items(), key=lambda item: item[1], reverse=True))\n",
    "    plt.plot(list(sorted_baseline.keys()), list(sorted_baseline.values()), marker='o', color='r', label='Baseline Population Percentage')\n",
    "    \n",
    "    plt.title(f\"Proportion of Each Ethnicity for Specialty: {specialty}\", fontsize=20)\n",
    "    plt.xlabel(\"Ethnicity\", fontsize=18)\n",
    "    plt.ylabel(\"Proportion\", fontsize=18)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba478980-6e3a-43fd-b6b3-91e5a9f4fca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Calculate the proportion of each \"ndl_ethnicity\" for each \"Specialty\"\n",
    "df_ethnicity_specialty = cohort_1_df.groupBy(\"Specialty\", \"ndl_ethnicity\").count()\n",
    "df_total_specialty = cohort_1_df.groupBy(\"Specialty\").count().withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "df_proportion = df_ethnicity_specialty.join(df_total_specialty, on=\"Specialty\")\n",
    "df_proportion = df_proportion.withColumn(\"proportion\", col(\"count\") / col(\"total_count\"))\n",
    "\n",
    "df_proportion_pandas = df_proportion.select(\"Specialty\", \"ndl_ethnicity\", \"proportion\").toPandas()\n",
    "\n",
    "# population data from https://www.ons.gov.uk/peoplepopulationandcommunity/culturalidentity/ethnicity/bulletins/ethnicgroupenglandandwales/census2021\n",
    "ethnicity_baseline_mapping = {\n",
    "    \"asian_background\": 0.097,\n",
    "    \"black_background\": 0.056,\n",
    "    \"white_background\": 0.79,\n",
    "    \"mixed_background\": 0.034,\n",
    "    \"other_background\": 0.023\n",
    "}\n",
    "\n",
    "df_proportion_pandas[\"baseline\"] = df_proportion_pandas[\"ndl_ethnicity\"].map(ethnicity_baseline_mapping)\n",
    "df_proportion_pandas[\"difference\"] = df_proportion_pandas[\"proportion\"] - df_proportion_pandas[\"baseline\"]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x=\"Specialty\", y=\"difference\", hue=\"ndl_ethnicity\", data=df_proportion_pandas, palette=\"Set2\")\n",
    "plt.title(\"Difference Between Proportion of Ethnicity and Baseline Across Each Specialty\")\n",
    "plt.xlabel(\"Specialty\")\n",
    "plt.ylabel(\"Difference in Proportion\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a24cad-3215-42d0-8c9a-1a92c34a1aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, expr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the count of distinct \"wlmds_rtt_start_date_conc3\" values for each \"wlmds_patient_id\"\n",
    "df_distinct_counts = cohort_1_df.groupBy(\"wlmds_patient_id\", \"Frailty_level\").agg(\n",
    "    countDistinct(\"wlmds_rtt_start_date_conc3\").alias(\"distinct_count\"),\n",
    "    expr(\"percentile_approx(ndl_wait_length, 0.5)\").alias(\"median_wait_length\")\n",
    ")\n",
    "\n",
    "# Filter out rows where distinct_count is 1\n",
    "df_distinct_counts = df_distinct_counts#.filter(col(\"distinct_count\") != 1)\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "df_distinct_counts_pandas = df_distinct_counts.toPandas()\n",
    "\n",
    "# Plot the frequency of each count and median wait length on the same chart with 2 y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Bar plot for median wait length with hue for \"Frailty_level\"\n",
    "sns.barplot(x=\"distinct_count\", y=\"median_wait_length\", hue=\"Frailty_level\", data=df_distinct_counts_pandas, ax=ax1, hue_order=[\"Fit\", \"Mild\", \"Moderate\", \"Severe\"])\n",
    "ax1.set_xlabel(\"Count of Distinct 'wlmds_rtt_start_date_conc3'\")\n",
    "ax1.set_ylabel(\"Median 'ndl_wait_length'\", color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.set_title(\"Frequency of Distinct 'wlmds_rtt_start_date_conc3' Counts and Median 'ndl_wait_length'\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08eb3c21-f4af-41e5-bf30-824995f57bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# Define the conditions for the two subsections\n",
    "condition_1 = (col(\"Frailty_level\") == \"Fit\") & (col(\"Age\") < 61) & (col(\"ndl_ltc\") == \"No LTCs\")\n",
    "condition_2 = ((col(\"Frailty_level\").isin(\"Severe\", \"Moderate\")) & (col(\"Age\") > 60) & \n",
    "               (col(\"ndl_ltc\").isin(\"Comorbidities\", \"Multimorbidities\", \"Single LTC\")))\n",
    "\n",
    "# Filter the DataFrame for each condition\n",
    "df_condition_1 = cohort_1_df.filter(condition_1)\n",
    "df_condition_2 = cohort_1_df.filter(condition_2)\n",
    "\n",
    "# Calculate the count of distinct \"wlmds_rtt_start_date_conc3\" values and median \"ndl_wait_length\" for each condition\n",
    "df_distinct_counts_1 = df_condition_1.groupBy(\"wlmds_patient_id\").agg(\n",
    "    countDistinct(\"wlmds_rtt_start_date_conc3\").alias(\"distinct_count\"),\n",
    "    expr(\"percentile_approx(ndl_wait_length, 0.5)\").alias(\"median_wait_length\")\n",
    ").withColumn(\"Subsection\", lit(\"Fit, Age < 61, No LTCs\"))\n",
    "\n",
    "df_distinct_counts_2 = df_condition_2.groupBy(\"wlmds_patient_id\").agg(\n",
    "    countDistinct(\"wlmds_rtt_start_date_conc3\").alias(\"distinct_count\"),\n",
    "    expr(\"percentile_approx(ndl_wait_length, 0.5)\").alias(\"median_wait_length\")\n",
    ").withColumn(\"Subsection\", lit(\"Severe/Moderate, Age > 60, Comorbidities/Multimorbidities/Single LTC\"))\n",
    "\n",
    "# Union the two DataFrames\n",
    "df_distinct_counts_combined = df_distinct_counts_1.union(df_distinct_counts_2)\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "df_distinct_counts_combined_pandas = df_distinct_counts_combined.toPandas()\n",
    "\n",
    "# Plot the median \"ndl_wait_length\" for each count of distinct \"wlmds_rtt_start_date_conc3\" values with subsections as hue\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "sns.barplot(x=\"distinct_count\", y=\"median_wait_length\", hue=\"Subsection\", data=df_distinct_counts_combined_pandas, ax=ax1)\n",
    "ax1.set_xlabel(\"Count of Distinct 'wlmds_rtt_start_date_conc3'\")\n",
    "ax1.set_ylabel(\"Median 'ndl_wait_length'\", color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.set_title(\"Median 'ndl_wait_length' for Each Count of Distinct 'wlmds_rtt_start_date_conc3' Values by Subsection\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "57300f40-a527-484f-98d3-d022043fd463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, countDistinct, expr, lit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the conditions for the two subsections\n",
    "condition_1 = (col(\"Frailty_level\") == \"Fit\") & (col(\"Age\") < 61) & (col(\"ndl_ltc\") == \"No LTCs\")\n",
    "condition_2 = ((col(\"Frailty_level\").isin(\"Severe\", \"Moderate\")) & (col(\"Age\") > 60) & \n",
    "               (col(\"ndl_ltc\").isin(\"Comorbidities\", \"Multimorbidities\", \"Single LTC\")))\n",
    "\n",
    "# Filter the DataFrame for each condition\n",
    "df_condition_1 = cohort_1_df.filter(condition_1)\n",
    "df_condition_2 = cohort_1_df.filter(condition_2)\n",
    "\n",
    "# Calculate the count of distinct \"wlmds_rtt_start_date_conc3\" values and median \"ndl_wait_length\" for each condition\n",
    "df_distinct_counts_1 = df_condition_1.groupBy(\"wlmds_patient_id\").agg(\n",
    "    countDistinct(\"wlmds_rtt_start_date_conc3\").alias(\"distinct_count\"),\n",
    "    expr(\"percentile_approx(ndl_wait_length, 0.5)\").alias(\"median_wait_length\")\n",
    ").withColumn(\"Subsection\", lit(\"Fit, Age < 61, No LTCs\"))\n",
    "\n",
    "df_distinct_counts_2 = df_condition_2.groupBy(\"wlmds_patient_id\").agg(\n",
    "    countDistinct(\"wlmds_rtt_start_date_conc3\").alias(\"distinct_count\"),\n",
    "    expr(\"percentile_approx(ndl_wait_length, 0.5)\").alias(\"median_wait_length\")\n",
    ").withColumn(\"Subsection\", lit(\"Severe/Moderate, Age > 60, Comorbidities/Multimorbidities/Single LTC\"))\n",
    "\n",
    "# Union the two DataFrames\n",
    "df_distinct_counts_combined = df_distinct_counts_1.union(df_distinct_counts_2)\n",
    "\n",
    "# Get distinct specialties\n",
    "distinct_specialties = cohort_1_df.select(\"Specialty\").distinct().collect()\n",
    "specialties = [row[\"Specialty\"] for row in distinct_specialties]\n",
    "\n",
    "# Loop through each specialty and create plots\n",
    "for specialty in specialties:\n",
    "    df_specialty = cohort_1_df.groupBy(\"wlmds_patient_id\").agg(\n",
    "        countDistinct(\"wlmds_rtt_start_date_conc3\").alias(\"distinct_count\")\n",
    "    ).filter(col(\"distinct_count\") > 1).join(cohort_1_df.filter(col(\"Specialty\") == specialty), \"wlmds_patient_id\")\n",
    "    \n",
    "    df_specialty = df_specialty.withColumnRenamed(\"distinct_count\", f\"{specialty.lower().replace(' ', '_')}_distinct_count\")\n",
    "    \n",
    "    df_specialty_combined = df_distinct_counts_combined.join(df_specialty, \"wlmds_patient_id\")\n",
    "    \n",
    "    df_specialty_combined_pandas = df_specialty_combined.select(\n",
    "        \"distinct_count\", \"median_wait_length\", \"Subsection\"\n",
    "    ).toPandas()\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    sns.barplot(x=\"distinct_count\", y=\"median_wait_length\", hue=\"Subsection\", data=df_specialty_combined_pandas, ax=ax1)\n",
    "    ax1.set_xlabel(\"Count of Distinct 'wlmds_rtt_start_date_conc3'\")\n",
    "    ax1.set_ylabel(\"Median 'ndl_wait_length'\", color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_title(f\"Median 'ndl_wait_length' for Each Count of Distinct 'wlmds_rtt_start_date_conc3' Values by Subsection ({specialty})\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c255e0ca-2e96-4cc2-9367-9a7022178324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, expr\n",
    "\n",
    "# Calculate the count of distinct \"wlmds_rtt_start_date_conc3\" values for each \"wlmds_patient_id\" and \"Specialty\"\n",
    "df_distinct_counts = cohort_1_df.groupBy(\"Specialty\", \"wlmds_patient_id\", \"Frailty_level\").agg(\n",
    "    countDistinct(\"wlmds_rtt_start_date_conc3\").alias(\"distinct_count\"),\n",
    "    expr(\"percentile_approx(ndl_wait_length, 0.5)\").alias(\"median_wait_length\")\n",
    ")\n",
    "\n",
    "# Filter out rows where distinct_count is 1\n",
    "df_distinct_counts = df_distinct_counts#.filter(col(\"distinct_count\") != 1)\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "df_distinct_counts_pandas = df_distinct_counts.toPandas()\n",
    "\n",
    "# Plot the frequency of each count and median wait length on the same chart with 2 y-axes for each Specialty\n",
    "specialties = df_distinct_counts_pandas[\"Specialty\"].unique()\n",
    "for specialty in specialties:\n",
    "    df_specialty = df_distinct_counts_pandas[df_distinct_counts_pandas[\"Specialty\"] == specialty]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Bar plot for median wait length with hue for \"Frailty_level\"\n",
    "    sns.barplot(x=\"distinct_count\", y=\"median_wait_length\", hue=\"Frailty_level\", data=df_specialty, ax=ax1)\n",
    "    ax1.set_xlabel(\"Count of Distinct 'wlmds_rtt_start_date_conc3'\")\n",
    "    ax1.set_ylabel(\"Median 'ndl_wait_length'\", color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_title(f\"Frequency of Distinct 'wlmds_rtt_start_date_conc3' Counts and Median 'ndl_wait_length' for {specialty}\")\n",
    "    \n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8264ef90-9990-48a6-986f-9259c6e0673c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cohort 2 Investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684059a5-c0cd-46e9-96be-8124c69946dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dummy data\n",
    "\n",
    "cohort_2_path=\"../Dummy_Data/Cohort_2_synth.xlsx\"\n",
    "cohort_2_df = pd.read_excel(cohort_2_path)\n",
    "\n",
    "cohort_2_df = spark.createDataFrame(cohort_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbaba75a-a6d8-4669-be34-df2b41068b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# real data\n",
    "# cohort_2_df = spark.read.format(\"parquet\").load(cohort_2_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c442e4-a5c5-4fef-baf5-5c30b05219e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (cohort_2_df.wlmds_status == 30) & (col(\"wlmds_type_changes_last\").contains(\"IRTT\")),    # treatment + admitted                  = 30 & IRTT\n",
    "    (cohort_2_df.wlmds_status == 30) & (~col(\"wlmds_type_changes_last\").contains(\"IRTT\")),   # treatment + not admitted              = 31 & no IRTT\n",
    "    (cohort_2_df.wlmds_status == 31) | (cohort_2_df.wlmds_status == 32),                    # non treatment, start monitoring       = 31 or 32\n",
    "    cohort_2_df.wlmds_status == 35,                                                         # non treatment, patient declines       = 35\n",
    "    cohort_2_df.wlmds_status == 34,                                                         # non treatment, decision to not treat  = 34\n",
    "    cohort_2_df.wlmds_status == 33,                                                         # non treatment, DNA                    = 33\n",
    "    (~cohort_2_df.wlmds_status.isin(30, 31, 32, 33, 34, 35, 36, 99)),                       # non treatment, other                  = not 30, 31, 32, 33, 34, 35, 36, 99\n",
    "    cohort_2_df.wlmds_status == 36,                                                         # non treatment, death                  = 36 \n",
    "    cohort_2_df.wlmds_status == 99,                                                         # unknown                               = 99\n",
    "    (cohort_2_df.wlmds_status == 99) & (col(\"wlmds_end_date_type_last\") == \"week_end_date\") # unknown, imputed end                  = 99 and week_end_date\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"treatment_admitted\",\n",
    "    \"treatment_non_admitted\",\n",
    "    \"non_treatment_monitoring\",\n",
    "    \"non_treatment_patient_declines\",\n",
    "    \"non_treatment_decision_not_to_treat\",\n",
    "    \"non_treatment_dna\",\n",
    "    \"non_treatment_other\",\n",
    "    \"non_treatment_death\",\n",
    "    \"unknown\",\n",
    "    \"unknown_imputed_end\"\n",
    "]\n",
    "\n",
    "reason_expr = when(conditions[0], labels[0])\n",
    "for cond, label in zip(conditions[1:], labels[1:]):\n",
    "    reason_expr = reason_expr.when(cond, label)\n",
    "\n",
    "cohort_2_df = cohort_2_df.withColumn(\"Reason\", reason_expr)\n",
    "#display(cohort_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c8940c-5742-4d49-80b0-9ecbd79e68f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = cohort_2_df.filter(col(\"wlmds_end_date_type_last\") == \"rtt_end_date\")\n",
    "\n",
    "# Group by Reason and calculate the median ndl_wait_length\n",
    "median_wait_length_df = filtered_df.groupBy(\"Reason\").agg(expr(\"percentile_approx(ndl_wait_length, 0.5)\").alias(\"median_ndl_wait_length\"))\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "median_wait_length_pd = median_wait_length_df.toPandas()\n",
    "\n",
    "# Define the order of the x-axis values\n",
    "order = [\n",
    "    \"treatment_admitted\", \n",
    "    \"treatment_non_admitted\", \n",
    "    \"non_treatment_monitoring\", \n",
    "    \"non_treatment_patient_declines\", \n",
    "    \"non_treatment_decision_not_to_treat\", \n",
    "    \"non_treatment_dna\", \n",
    "    \"non_treatment_death\", \n",
    "    \"unknown\"\n",
    "]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "barplot = sns.barplot(x=\"Reason\", y=\"median_ndl_wait_length\", data=median_wait_length_pd, order=order)\n",
    "\n",
    "# Highlight bars with median wait length above 100 days\n",
    "for index, reason in enumerate(order):\n",
    "    if median_wait_length_pd[median_wait_length_pd[\"Reason\"] == reason][\"median_ndl_wait_length\"].values[0] > 100:\n",
    "        barplot.patches[index].set_color('red')\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\", fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel(\"Reason\", fontsize=18)\n",
    "plt.ylabel(\"Median NDL Wait Length (days)\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fecd3b8-252a-4319-ba2f-b6025f33a2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5a883c9-5fd8-4af8-ba13-5a92d5baf365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Objective 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e3db30-d902-4c1f-ac55-15d1fc4a2a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dummy data\n",
    "cohort_1_path=\"../Dummy_Data/Cohort_1_synth.xlsx\"\n",
    "cohort_1_df = pd.read_excel(cohort_1_path)\n",
    "\n",
    "cohort_1_df = spark.createDataFrame(cohort_1_df)\n",
    "\n",
    "#real data\n",
    "#cohort_1_df = spark.read.format(\"parquet\").load(cohort_1_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed032d9-2914-4990-a61c-2c0a647b0e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cohort_1_inpatient_df = cohort_1_df.filter(cohort_1_df.ndl_type == \"inpatient\")\n",
    "cohort_1_outpatient_df = cohort_1_df.filter(cohort_1_df.ndl_type == \"outpatient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a66a0ee-4a18-41a6-815f-26f7eece08de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_11_1_all = calculate_wait_band_distribution(cohort_1_df, \"Specialty\")\n",
    "df_11_2_all = calculate_wait_band_distribution(cohort_1_df, \"Sex\")\n",
    "df_11_3_all = calculate_wait_band_distribution(cohort_1_df, \"ndl_age_band\")\n",
    "df_11_4_all = calculate_wait_band_distribution(cohort_1_df, \"ndl_imd_quantile\")\n",
    "df_11_5_all = calculate_wait_band_distribution(cohort_1_df, \"ndl_ethnicity\")\n",
    "df_11_6_all = calculate_wait_band_distribution(cohort_1_df, \"Frailty_Level\")\n",
    "df_11_7_all = calculate_wait_band_distribution(cohort_1_df, \"ndl_ltc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "409cf611-9f1a-4b8f-b967-3c01a0117f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_11_1_all = rename_and_add_column(df_11_1_all, \"Value\")\n",
    "df_11_2_all = rename_and_add_column(df_11_2_all, \"Value\")\n",
    "df_11_3_all = rename_and_add_column(df_11_3_all, \"Value\")\n",
    "df_11_4_all = rename_and_add_column(df_11_4_all, \"Value\")\n",
    "df_11_5_all = rename_and_add_column(df_11_5_all, \"Value\")\n",
    "df_11_6_all = rename_and_add_column(df_11_6_all, \"Value\")\n",
    "df_11_7_all = rename_and_add_column(df_11_7_all, \"Value\")\n",
    "\n",
    "combined_df11_all = df_11_1_all.unionByName(df_11_2_all).unionByName(df_11_3_all).unionByName(df_11_4_all).unionByName(df_11_5_all).unionByName(df_11_6_all).unionByName(df_11_7_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "474a5511-e968-4610-bfcc-be256b550858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_11_1_ip = calculate_wait_band_distribution(cohort_1_inpatient_df, \"Specialty\")\n",
    "df_11_2_ip = calculate_wait_band_distribution(cohort_1_inpatient_df, \"Sex\")\n",
    "df_11_3_ip = calculate_wait_band_distribution(cohort_1_inpatient_df, \"ndl_age_band\")\n",
    "df_11_4_ip = calculate_wait_band_distribution(cohort_1_inpatient_df, \"ndl_imd_quantile\")\n",
    "df_11_5_ip = calculate_wait_band_distribution(cohort_1_inpatient_df, \"ndl_ethnicity\")\n",
    "df_11_6_ip = calculate_wait_band_distribution(cohort_1_inpatient_df, \"Frailty_Level\")\n",
    "df_11_7_ip = calculate_wait_band_distribution(cohort_1_inpatient_df, \"ndl_ltc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7106d205-e6fc-4cbd-afe5-332bc7d18607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_11_1_ip = rename_and_add_column(df_11_1_ip, \"Value\")\n",
    "df_11_2_ip = rename_and_add_column(df_11_2_ip, \"Value\")\n",
    "df_11_3_ip = rename_and_add_column(df_11_3_ip, \"Value\")\n",
    "df_11_4_ip = rename_and_add_column(df_11_4_ip, \"Value\")\n",
    "df_11_5_ip = rename_and_add_column(df_11_5_ip, \"Value\")\n",
    "df_11_6_ip = rename_and_add_column(df_11_6_ip, \"Value\")\n",
    "df_11_7_ip = rename_and_add_column(df_11_7_ip, \"Value\")\n",
    "\n",
    "combined_df11_ip = df_11_1_ip.unionByName(df_11_2_ip).unionByName(df_11_3_ip).unionByName(df_11_4_ip).unionByName(df_11_5_ip).unionByName(df_11_6_ip).unionByName(df_11_7_ip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba8c656-1940-4d86-a450-0a2c682ae396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_11_1_op = calculate_wait_band_distribution(cohort_1_outpatient_df, \"Specialty\")\n",
    "df_11_2_op = calculate_wait_band_distribution(cohort_1_outpatient_df, \"Sex\")\n",
    "df_11_3_op = calculate_wait_band_distribution(cohort_1_outpatient_df, \"ndl_age_band\")\n",
    "df_11_4_op = calculate_wait_band_distribution(cohort_1_outpatient_df, \"ndl_imd_quantile\")\n",
    "df_11_5_op = calculate_wait_band_distribution(cohort_1_outpatient_df, \"ndl_ethnicity\")\n",
    "df_11_6_op = calculate_wait_band_distribution(cohort_1_outpatient_df, \"Frailty_Level\")\n",
    "df_11_7_op = calculate_wait_band_distribution(cohort_1_outpatient_df, \"ndl_ltc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51dd2d4a-f5cb-4696-aafc-7522409273e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_11_1_op = rename_and_add_column(df_11_1_op, \"Value\")\n",
    "df_11_2_op = rename_and_add_column(df_11_2_op, \"Value\")\n",
    "df_11_3_op = rename_and_add_column(df_11_3_op, \"Value\")\n",
    "df_11_4_op = rename_and_add_column(df_11_4_op, \"Value\")\n",
    "df_11_5_op = rename_and_add_column(df_11_5_op, \"Value\")\n",
    "df_11_6_op = rename_and_add_column(df_11_6_op, \"Value\")\n",
    "df_11_7_op = rename_and_add_column(df_11_7_op, \"Value\")\n",
    "\n",
    "combined_df11_op = df_11_1_op.unionByName(df_11_2_op).unionByName(df_11_3_op).unionByName(df_11_4_op).unionByName(df_11_5_op).unionByName(df_11_6_op).unionByName(df_11_7_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da314c74-66fa-43c5-b9a6-32cf9d132c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_12_1 = calculate_cross_wait_band_distribution(cohort_1_df, \"ndl_age_band\", \"Sex\")\n",
    "df_12_2 = calculate_cross_wait_band_distribution(cohort_1_df, \"ndl_imd_quantile\", \"Sex\")\n",
    "df_12_3 = calculate_cross_wait_band_distribution(cohort_1_df, \"ndl_imd_quantile\", \"ndl_age_band\")\n",
    "# Requested tables for tom:\n",
    "# imd x frailty, imd x ltc, ethnicity x frailty, ethnicity x ltc, imd x ethnicity\n",
    "df_12_4 = calculate_cross_wait_band_distribution(cohort_1_df, \"ndl_imd_quantile\", \"Frailty_Level\")\n",
    "df_12_5 = calculate_cross_wait_band_distribution(cohort_1_df, \"ndl_imd_quantile\", \"ndl_ltc\")\n",
    "df_12_6 = calculate_cross_wait_band_distribution(cohort_1_df, \"ndl_ethnicity\", \"Frailty_Level\")\n",
    "df_12_7 = calculate_cross_wait_band_distribution(cohort_1_df, \"ndl_ethnicity\", \"ndl_ltc\")\n",
    "df_12_8 = calculate_cross_wait_band_distribution(cohort_1_df, \"ndl_imd_quantile\", \"ndl_ethnicity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c4ddfa-bade-4453-b2ba-0a93f9ee3ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_12_1 = rename_and_add_column_cross(df_12_1, \"Value 1\", \"Value 2\")\n",
    "df_12_2 = rename_and_add_column_cross(df_12_2, \"Value 1\", \"Value 2\")\n",
    "df_12_3 = rename_and_add_column_cross(df_12_3, \"Value 1\", \"Value 2\")\n",
    "df_12_4 = rename_and_add_column_cross(df_12_4, \"Value 1\", \"Value 2\")\n",
    "df_12_5 = rename_and_add_column_cross(df_12_5, \"Value 1\", \"Value 2\")\n",
    "df_12_6 = rename_and_add_column_cross(df_12_6, \"Value 1\", \"Value 2\")\n",
    "df_12_7 = rename_and_add_column_cross(df_12_7, \"Value 1\", \"Value 2\")\n",
    "df_12_8 = rename_and_add_column_cross(df_12_8, \"Value 1\", \"Value 2\")\n",
    "\n",
    "combined_df12 = df_12_1.unionByName(df_12_2).unionByName(df_12_3).unionByName(df_12_4).unionByName(df_12_5).unionByName(df_12_6).unionByName(df_12_7).unionByName(df_12_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32e58e88-242b-4276-8e29-c47c7ecb8a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_13_1 = calculate_wait_length_statistics(cohort_1_df, \"ndl_wait_length\", \"Specialty\")\n",
    "df_13_2 = calculate_wait_length_statistics(cohort_1_df, \"ndl_wait_length\", \"Sex\")\n",
    "df_13_3 = calculate_wait_length_statistics(cohort_1_df, \"ndl_wait_length\", \"ndl_age_band\")\n",
    "df_13_4 = calculate_wait_length_statistics(cohort_1_df, \"ndl_wait_length\", \"ndl_imd_quantile\")\n",
    "df_13_5 = calculate_wait_length_statistics(cohort_1_df, \"ndl_wait_length\", \"ndl_ethnicity\")\n",
    "df_13_6 = calculate_wait_length_statistics(cohort_1_df, \"ndl_wait_length\", \"Frailty_level\")\n",
    "df_13_7 = calculate_wait_length_statistics(cohort_1_df, \"ndl_wait_length\", \"ndl_ltc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ff4268-8e19-414b-88c4-ea0c7798bece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_13_1 = rename_and_add_column(df_13_1, \"Value\")\n",
    "df_13_2 = rename_and_add_column(df_13_2, \"Value\")\n",
    "df_13_3 = rename_and_add_column(df_13_3, \"Value\")\n",
    "df_13_4 = rename_and_add_column(df_13_4, \"Value\")\n",
    "df_13_5 = rename_and_add_column(df_13_5, \"Value\")\n",
    "df_13_6 = rename_and_add_column(df_13_6, \"Value\")\n",
    "df_13_7 = rename_and_add_column(df_13_7, \"Value\")\n",
    "\n",
    "combined_df13 = df_13_1.unionByName(df_13_2).unionByName(df_13_3).unionByName(df_13_4).unionByName(df_13_5).unionByName(df_13_6).unionByName(df_13_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c77a32-2afc-4718-a70b-6614b8ef58f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_14_1 = calculate_wait_length_statistics_cross(cohort_1_df, \"ndl_wait_length\", \"ndl_age_band\", \"Sex\")\n",
    "df_14_2 = calculate_wait_length_statistics_cross(cohort_1_df, \"ndl_wait_length\", \"ndl_imd_quantile\", \"Sex\")\n",
    "df_14_3 = calculate_wait_length_statistics_cross(cohort_1_df, \"ndl_wait_length\", \"ndl_age_band\", \"ndl_imd_quantile\")\n",
    "\n",
    "# Requested tables for tom:\n",
    "# imd x frailty, imd x ltc, ethnicity x frailty, ethnicity x ltc, imd x ethnicity\n",
    "df_14_4 = calculate_wait_length_statistics_cross(cohort_1_df, \"ndl_wait_length\", \"ndl_imd_quantile\", \"Frailty_Level\")\n",
    "df_14_5 = calculate_wait_length_statistics_cross(cohort_1_df, \"ndl_wait_length\", \"ndl_imd_quantile\", \"ndl_ltc\")\n",
    "df_14_6 = calculate_wait_length_statistics_cross(cohort_1_df, \"ndl_wait_length\", \"ndl_ethnicity\", \"Frailty_Level\")\n",
    "df_14_7 = calculate_wait_length_statistics_cross(cohort_1_df, \"ndl_wait_length\", \"ndl_ethnicity\", \"ndl_ltc\")\n",
    "df_14_8 = calculate_wait_length_statistics_cross(cohort_1_df, \"ndl_wait_length\", \"ndl_imd_quantile\", \"ndl_ethnicity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c018f14f-9bdf-4471-a1cb-616bfcdf2338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_14_1 = rename_and_add_column_cross(df_14_1, \"Value 1\", \"Value 2\")\n",
    "df_14_2 = rename_and_add_column_cross(df_14_2, \"Value 1\", \"Value 2\")\n",
    "df_14_3 = rename_and_add_column_cross(df_14_3, \"Value 1\", \"Value 2\")\n",
    "df_14_4 = rename_and_add_column_cross(df_14_4, \"Value 1\", \"Value 2\")\n",
    "df_14_5 = rename_and_add_column_cross(df_14_5, \"Value 1\", \"Value 2\")\n",
    "df_14_6 = rename_and_add_column_cross(df_14_6, \"Value 1\", \"Value 2\")\n",
    "df_14_7 = rename_and_add_column_cross(df_14_7, \"Value 1\", \"Value 2\")\n",
    "df_14_8 = rename_and_add_column_cross(df_14_8, \"Value 1\", \"Value 2\")\n",
    "\n",
    "combined_df14 = df_14_1.unionByName(df_14_2).unionByName(df_14_3).unionByName(df_14_4).unionByName(df_14_5).unionByName(df_14_6).unionByName(df_14_7).unionByName(df_14_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb5ce1b-322b-4514-89b0-e9f8536a7fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "objective_1_1_all = objective_1_1_all_link\n",
    "objective_1_1_ip = objective_1_1_ip_link\n",
    "objective_1_1_op = objective_1_1_op_link\n",
    "objective_1_2 = objective_1_2_link\n",
    "objective_1_3 = objective_1_3_link\n",
    "objective_1_4 = objective_1_4_link\n",
    "\n",
    "#combined_df11_all.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_1_1_all)\n",
    "#combined_df11_ip.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_1_1_ip)\n",
    "#combined_df11_op.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_1_1_op)\n",
    "#combined_df12.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_1_2)\n",
    "#combined_df13.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_1_3)\n",
    "#combined_df14.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_1_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c199466f-ead6-40fe-b891-5ca406052ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Objective 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b06f3e1-7954-4e94-adbc-38f0e07fcbf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dummy data\n",
    "\n",
    "cohort_2_path=\"../Dummy_Data/Cohort_2_synth.xlsx\"\n",
    "cohort_2_df = pd.read_excel(cohort_2_path)\n",
    "\n",
    "cohort_2_df = spark.createDataFrame(cohort_2_df)\n",
    "\n",
    "#real data\n",
    "# cohort_2_df = spark.read.format(\"parquet\").load(cohort_2_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf0af128-03ec-4098-8652-f334e5b1d6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_21_1 = objective2_table(cohort_2_df, \"Specialty\")\n",
    "df_21_2 = objective2_table(cohort_2_df, \"Sex\")\n",
    "df_21_3 = objective2_table(cohort_2_df, \"ndl_age_band\")\n",
    "df_21_4 = objective2_table(cohort_2_df, \"ndl_imd_quantile\")\n",
    "df_21_5 = objective2_table(cohort_2_df, \"ndl_ethnicity\")\n",
    "df_21_6 = objective2_table(cohort_2_df, \"Frailty_level\")\n",
    "df_21_7 = objective2_table(cohort_2_df, \"ndl_ltc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f374fc-d89a-4344-9a1d-f3eee583d30e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_21_1 = rename_and_add_column(df_21_1, \"Value\")\n",
    "df_21_2 = rename_and_add_column(df_21_2, \"Value\")\n",
    "df_21_3 = rename_and_add_column(df_21_3, \"Value\")\n",
    "df_21_4 = rename_and_add_column(df_21_4, \"Value\")\n",
    "df_21_5 = rename_and_add_column(df_21_5, \"Value\")\n",
    "df_21_6 = rename_and_add_column(df_21_6, \"Value\")\n",
    "df_21_7 = rename_and_add_column(df_21_7, \"Value\")\n",
    "\n",
    "combined_df_2 = df_21_1.unionByName(df_21_2).unionByName(df_21_3).unionByName(df_21_4).unionByName(df_21_5).unionByName(df_21_6).unionByName(df_21_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420f09a9-3a39-4bfe-9945-d463ab25edb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "objective_2 = objective_2_link\n",
    "\n",
    "#combined_df_2.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c878d0-eaa3-4125-b07c-182bc2519c38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Objective 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1e07d3-ae1d-41fa-86f3-96de6081dc5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dummy data\n",
    "cohort_3_path=\"../Dummy_Data/Cohort_3_synth.xlsx\"\n",
    "cohort_3_df = pd.read_excel(cohort_3_path)\n",
    "\n",
    "cohort_3_df = spark.createDataFrame(cohort_3_df).na.fill(0)\n",
    "\n",
    "# real data\n",
    "#cohort_3_df = spark.read.format('parquet').load(cohort_3_link).na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff72abc3-5bcc-439e-b42e-d37c6f63505c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "use_prefixes = [\n",
    "    \"gp_healthcare_use_sum\",\n",
    "    \"u111_healthcare_use_sum\",\n",
    "    \"u999_healthcare_use_sum\",\n",
    "    \"u00H_healthcare_use_sum\",\n",
    "    \"ae_healthcare_use_sum\",\n",
    "    \"nel_healthcare_use_sum\",\n",
    "    \"el_healthcare_use_sum\",\n",
    "    \"op_healthcare_use_sum\",\n",
    "    \"all_pres_sum\",\n",
    "    \"antib_pres_sum\",\n",
    "    \"antidep_pres_sum\",\n",
    "    \"pain_pres_sum\",\n",
    "    \"sick_note_sum\"\n",
    "]\n",
    "\n",
    "investigation_cols = [\n",
    "    \"Specialty\",\n",
    "    \"Sex\",\n",
    "    \"ndl_age_band\",\n",
    "    \"ndl_imd_quantile\",\n",
    "    \"ndl_ethnicity\",\n",
    "    \"Frailty_Level\",\n",
    "    \"ndl_ltc\"\n",
    "]\n",
    "\n",
    "result_dfs = {}\n",
    "\n",
    "for prefix in use_prefixes:\n",
    "    dfs = [rename_and_add_column(objective_3_stats(cohort_3_df, col, prefix), \"Value\") for col in investigation_cols]\n",
    "    result_df = reduce(lambda df1, df2: df1.unionByName(df2), dfs)\n",
    "    result_dfs[prefix] = result_df\n",
    "\n",
    "df_31_1_use = result_dfs[\"gp_healthcare_use_sum\"]\n",
    "df_31_2_use = result_dfs[\"u111_healthcare_use_sum\"]\n",
    "df_31_3_use = result_dfs[\"u999_healthcare_use_sum\"]\n",
    "df_31_4_use = result_dfs[\"u00H_healthcare_use_sum\"]\n",
    "df_31_5_use = result_dfs[\"ae_healthcare_use_sum\"]\n",
    "df_31_6_use = result_dfs[\"nel_healthcare_use_sum\"]\n",
    "df_31_7_use = result_dfs[\"el_healthcare_use_sum\"]\n",
    "df_31_8_use = result_dfs[\"op_healthcare_use_sum\"]\n",
    "df_31_9_use = result_dfs[\"all_pres_sum\"]\n",
    "df_31_10_use = result_dfs[\"antib_pres_sum\"]\n",
    "df_31_11_use = result_dfs[\"antidep_pres_sum\"]\n",
    "df_31_12_use = result_dfs[\"pain_pres_sum\"]\n",
    "df_31_13_use = result_dfs[\"sick_note_sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17253106-1838-4de7-bbf0-ded12cf30eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "objective_3_1_gp = objective_3_1_gp_link\n",
    "objective_3_1_111 = objective_3_1_111_link\n",
    "objective_3_1_999 = objective_3_1_999_link\n",
    "objective_3_1_ooh = objective_3_1_ooh_link\n",
    "objective_3_1_ae = objective_3_1_ae_link\n",
    "objective_3_1_nel = objective_3_1_nel_link\n",
    "objective_3_1_el = objective_3_1_el_link\n",
    "objective_3_1_op = objective_3_1_op_link\n",
    "objective_3_1_all_pres = objective_3_1_all_pres_link\n",
    "objective_3_1_antib = objective_3_1_antib_link\n",
    "objective_3_1_antidep = objective_3_1_antidep_link\n",
    "objective_3_1_pain = objective_3_1_pain_link\n",
    "objective_3_1_sick_note = objective_3_1_sick_note_link\n",
    "\n",
    "#df_31_1_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_gp)\n",
    "#df_31_2_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_111)\n",
    "#df_31_3_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_999)\n",
    "#df_31_4_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_ooh)\n",
    "#df_31_5_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_ae)\n",
    "#df_31_6_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_nel)\n",
    "#df_31_7_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_el)\n",
    "#df_31_8_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_op)\n",
    "#df_31_9_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_all_pres)\n",
    "#df_31_10_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_antib)\n",
    "#df_31_11_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_antidep)\n",
    "#df_31_12_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_pain)\n",
    "#df_31_13_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_sick_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c698730b-d81f-4f2f-abd1-9bb6dab69a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cost_prefixes = [\n",
    "    \"op_Total_Cost\",\n",
    "    \"ae_Total_Cost\",\n",
    "    \"gp_Total_Cost\",\n",
    "    \"el_Total_Cost\"\n",
    "]\n",
    "\n",
    "result_dfs = {}\n",
    "\n",
    "for prefix in cost_prefixes:\n",
    "    dfs = [rename_and_add_column(objective_3_stats(cohort_3_df, col, prefix), \"Value\") for col in investigation_cols]\n",
    "    result_df = reduce(lambda df1, df2: df1.unionByName(df2), dfs)\n",
    "    result_dfs[prefix] = result_df\n",
    "\n",
    "df_31_1_cost = result_dfs[\"op_Total_Cost\"]\n",
    "df_31_2_cost = result_dfs[\"ae_Total_Cost\"]\n",
    "df_31_3_cost = result_dfs[\"gp_Total_Cost\"]\n",
    "df_31_4_cost = result_dfs[\"el_Total_Cost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9d25a2-241f-423c-a9f6-31b1a588d9c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "objective_3_1_op_cost = objective_3_1_op_cost_link\n",
    "objective_3_1_ae_cost = objective_3_1_ae_cost_link\n",
    "objective_3_1_gp_cost = objective_3_1_gp_cost_link\n",
    "objective_3_1_el_cost = objective_3_1_el_cost_link\n",
    "\n",
    "#df_31_1_cost.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_op_cost)\n",
    "#df_31_2_cost.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_ae_cost)\n",
    "#df_31_3_cost.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_gp_cost)\n",
    "#df_31_4_cost.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_1_el_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe1a0a3-f979-464c-8711-add04c30c0a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ob_3_2_cols = [\n",
    "    \"ndl_wait_band\"\n",
    "]\n",
    "\n",
    "for prefix in use_prefixes:\n",
    "    dfs = [rename_and_add_column(objective_3_stats(cohort_3_df, col, prefix), \"Value\") for col in ob_3_2_cols]\n",
    "    result_df = reduce(lambda df1, df2: df1.unionByName(df2), dfs)\n",
    "    result_dfs[prefix] = result_df\n",
    "\n",
    "df_32_1_use = result_dfs[\"gp_healthcare_use_sum\"]\n",
    "df_32_2_use = result_dfs[\"u111_healthcare_use_sum\"]\n",
    "df_32_3_use = result_dfs[\"u999_healthcare_use_sum\"]\n",
    "df_32_4_use = result_dfs[\"u00H_healthcare_use_sum\"]\n",
    "df_32_5_use = result_dfs[\"ae_healthcare_use_sum\"]\n",
    "df_32_6_use = result_dfs[\"nel_healthcare_use_sum\"]\n",
    "df_32_7_use = result_dfs[\"el_healthcare_use_sum\"]\n",
    "df_32_8_use = result_dfs[\"op_healthcare_use_sum\"]\n",
    "df_32_9_use = result_dfs[\"all_pres_sum\"]\n",
    "df_32_10_use = result_dfs[\"antib_pres_sum\"]\n",
    "df_32_11_use = result_dfs[\"antidep_pres_sum\"]\n",
    "df_32_12_use = result_dfs[\"pain_pres_sum\"]\n",
    "df_32_13_use = result_dfs[\"sick_note_sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f3054c-40fd-4b79-b599-711362ebfeaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "objective_3_2_gp = objective_3_2_gp_link\n",
    "objective_3_2_111 = objective_3_2_111_link\n",
    "objective_3_2_999 = objective_3_2_999_link\n",
    "objective_3_2_ooh = objective_3_2_ooh_link\n",
    "objective_3_2_ae = objective_3_2_ae_link\n",
    "objective_3_2_nel = objective_3_2_nel_link\n",
    "objective_3_2_el = objective_3_2_el_link\n",
    "objective_3_2_op = objective_3_2_op_link\n",
    "objective_3_2_all_pres = objective_3_2_all_pres_link\n",
    "objective_3_2_antib = objective_3_2_antib_link\n",
    "objective_3_2_antidep = objective_3_2_antidep_link\n",
    "objective_3_2_pain = objective_3_2_pain_link\n",
    "objective_3_2_sick_note = objective_3_2_sick_note_link\n",
    "\n",
    "#df_32_1_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_gp)\n",
    "#df_32_2_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_111)\n",
    "#df_32_3_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_999)\n",
    "#df_32_4_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_ooh)\n",
    "#df_32_5_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_ae)\n",
    "#df_32_6_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_nel)\n",
    "#df_32_7_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_el)\n",
    "#df_32_8_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_op)\n",
    "#df_32_9_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_all_pres)\n",
    "#df_32_10_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_antib)\n",
    "#df_32_11_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_antidep)\n",
    "#df_32_12_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_pain)\n",
    "#df_32_13_use.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_sick_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d828965-315d-4891-80c9-ae3e27aa1bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ob_3_2_cols = [\n",
    "    \"ndl_wait_band\"\n",
    "]\n",
    "\n",
    "for prefix in cost_prefixes:\n",
    "    dfs = [rename_and_add_column(objective_3_stats(cohort_3_df, col, prefix), \"Value\") for col in ob_3_2_cols]\n",
    "    result_df = reduce(lambda df1, df2: df1.unionByName(df2), dfs)\n",
    "    result_dfs[prefix] = result_df\n",
    "\n",
    "df_32_1_cost = result_dfs[\"op_Total_Cost\"]\n",
    "df_32_2_cost = result_dfs[\"ae_Total_Cost\"]\n",
    "df_32_3_cost = result_dfs[\"gp_Total_Cost\"]\n",
    "df_32_4_cost = result_dfs[\"el_Total_Cost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dddb9b8b-998f-40e7-9e1a-4567ea033b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "objective_3_2_op_cost = objective_3_2_op_cost_link\n",
    "objective_3_2_ae_cost = objective_3_2_ae_cost_link\n",
    "objective_3_2_gp_cost = objective_3_2_gp_cost_link\n",
    "objective_3_2_el_cost = objective_3_2_el_cost_link\n",
    "\n",
    "#df_32_1_cost.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_op_cost)\n",
    "#df_32_2_cost.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_ae_cost)\n",
    "#df_32_3_cost.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_gp_cost)\n",
    "#df_32_4_cost.write.format('parquet').mode('overwrite').option('overwriteSchema','True').save(objective_3_2_el_cost)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5929057708227328,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Objectives 1-3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
