{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6862c799-b6b7-4692-8e30-1b1edf323f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427e0eb0-1558-4a5a-9fd7-42c92e454cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, datediff, date_add, lit, row_number, abs as abs_,\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "\n",
    "# Data Handling & ML (Python)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# StatsModels\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df760662-6b25-405c-a0a0-089776b2b400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PropensityScoreMatching\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e89598-39b3-43a0-b73a-400cba9bb3e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_data_function_M(df, intervention_group, control_group, recovery):\n",
    "    # Convert max follow-up weeks to days\n",
    "    \n",
    "    # Filter and add time periods based on individual waiting time\n",
    "    df_filtered = (\n",
    "        df.filter((col(\"group\") == intervention_group) | (col(\"group\") == control_group))\n",
    "        .withColumn(\"start_of_reference_period\", when(col(\"epp_rtt_end_date\") < date_add(col(\"epp_rtt_start_date\"),85), col(\"epp_rtt_end_date\")).otherwise(date_add(col(\"epp_rtt_start_date\"), 85)))\n",
    "        .withColumn(\"end_of_reference_period\", date_add(col(\"start_of_reference_period\"), 600))  # Define recovery period if needed\n",
    "        .filter((col(\"days\") >= col(\"epp_rtt_start_date\"))  & (col(\"days\")<=col(\"end_of_reference_period\")))\n",
    "        .withColumn(\n",
    "            \"time_period\",\n",
    "            when(col(\"days\") <= col(\"start_of_reference_period\"), 0)  # Waiting period\n",
    "            .otherwise(1)  # Post-recovery period\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd908c0-62b3-40f2-a6f3-4fe7e4f2601b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def group_data_function(df_filtered, intervention_group, control_group, colum_hc, col_list):\n",
    "    # Filter out those who do not have full 1 year follow-up\n",
    "    df_filtered = df_filtered.filter(datediff(col(\"end_of_reference_period\"), col(\"start_of_reference_period\")) >= 600)\n",
    "    \n",
    "    # Group and aggregate\n",
    "    df_grouped_by_time_period = (\n",
    "        df_filtered.groupBy(\"epp_pid\", \"epp_rtt_start_date\", \"epp_tfc\", \"time_period\", \"group\", *col_list)\n",
    "        .agg(F.sum(colum_hc).alias('total_hc_use'))\n",
    "        .withColumn(\n",
    "            \"treated\",\n",
    "            when(col(\"group\") == control_group, 0).when(col(\"group\") == intervention_group, 1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_grouped = df_grouped_by_time_period.withColumn(\n",
    "        'total_hc_use', \n",
    "        when(col('total_hc_use').isNull(), 0).otherwise(col('total_hc_use'))\n",
    "    ).withColumn(\n",
    "        'avg_weekly_use', \n",
    "        when(col(\"time_period\") == 1, (F.col('total_hc_use') / 600 * 7).cast('double'))\n",
    "        .otherwise((F.col('total_hc_use') / 85 *7).cast('double'))\n",
    "    )\n",
    "\n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e35a9f4f-e7ce-4036-aab5-0b53375ed2a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gb_data = simulate_waiting_list_data()\n",
    "gb_data = gb_data.withColumn(\"wt\", datediff(col(\"epp_rtt_end_date\"), col(\"epp_rtt_start_date\")))\n",
    "\n",
    "waiting_list_df = gb_data.withColumn(\n",
    "    \"group\",\n",
    "    when((col(\"wt\") <= 84), \"<= 12 weeks\")\n",
    "    .when((col(\"wt\") > 84) & (col(\"wt\") <= 126), \"<= 18 weeks\")\n",
    "    .otherwise(\"> 18 weeks\")\n",
    ")\n",
    "waiting_list_df = waiting_list_df.filter((col(\"wt\") > 84) )\n",
    "#display(waiting_list_df.groupby(\"group\").count())\n",
    "\n",
    "waiting_list_df = waiting_list_df.filter(col(\"epp_referral_priority\")!='cancer' )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59b89302-484e-4db3-a490-9ba14a646479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inter_group=\"> 18 weeks\"\n",
    "df_30_weeks_filtered = filter_data_function_M(\n",
    "    df=waiting_list_df, \n",
    "    intervention_group=inter_group, \n",
    "    control_group=\"<= 18 weeks\",\n",
    "    recovery=28  # Default recovery period\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc5c978-ffa9-4e9e-93f3-e2d7741b65a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_30_weeks_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27eeccbb-8ac1-4ae5-ad4d-a5bfc0b77633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "columns = [\"ndl_age_band\", \"ndl_imd_quantile\", \"ndl_ethnicity\", \"ndl_ltc\", \"Sex\", \"Frailty_level\"]\n",
    "\n",
    "# Replace null values with \"unknown\" in the specified columns\n",
    "for column in columns:\n",
    "    df_30_weeks_filtered = df_30_weeks_filtered.withColumn(column, when(col(column).isNull(), \"unknown\").otherwise(col(column)))\n",
    "\n",
    "df_weeks_grouped = group_data_function(\n",
    "    df_filtered=df_30_weeks_filtered,\n",
    "    intervention_group=inter_group,\n",
    "    control_group=\"<= 18 weeks\", \n",
    "    colum_hc=\"gp_healthcare_use\",\n",
    "    col_list=columns\n",
    ")\n",
    "display(df_weeks_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd2b2a43-5d8b-460d-a03b-4baae8dfef79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "categorical_cols = [\"ndl_age_band\", \"ndl_imd_quantile\", \"ndl_ethnicity\", \"ndl_ltc\", \"Sex\", \"Frailty_level\"]\n",
    "\n",
    "# Index and encode categorical columns\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\").fit(df_weeks_grouped) for col in categorical_cols]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = pipeline.fit(df_weeks_grouped).transform(df_weeks_grouped)\n",
    "\n",
    "# Assemble features for logistic regression\n",
    "feature_cols = [col + \"_index\" for col in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df_indexed).select(\"features\", \"treated\", \"epp_pid\", \"epp_rtt_start_date\", \"epp_tfc\" ,\"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71982b89-4ebb-4044-a3e3-c94c43c897c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit logistic regression to predict treatment probability\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"treated\", probabilityCol=\"propensity_score\")\n",
    "lr_model = lr.fit(df_assembled)\n",
    "\n",
    "# Get propensity scores\n",
    "df_matched = lr_model.transform(df_assembled).select(\"epp_pid\", \"epp_rtt_start_date\", \"epp_tfc\" ,\"group\", \"propensity_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7dd0ad6-f3ba-4b8c-a01e-d3beb36712ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split into treatment and control groups\n",
    "treated_df = df_matched.filter(col(\"group\") == \"> 18 weeks\")\n",
    "control_df = df_matched.filter(col(\"group\") == \"<= 18 weeks\")\n",
    "\n",
    "# UDF to extract the first value from a vector\n",
    "def extract_first_element(vector):\n",
    "    if isinstance(vector, DenseVector) or isinstance(vector, SparseVector):\n",
    "        return float(vector[0])  # Get the first element\n",
    "    return None\n",
    "\n",
    "# Register UDF to convert vector to double\n",
    "extract_first_element_udf = udf(extract_first_element, DoubleType())\n",
    "\n",
    "# Apply UDF to extract numeric scores\n",
    "treated_df = treated_df.withColumn(\"propensity_score_num\", extract_first_element_udf(col(\"propensity_score\")))\n",
    "control_df = control_df.withColumn(\"control_score_num\", extract_first_element_udf(col(\"propensity_score\")))\n",
    "\n",
    "# Rename control columns to avoid conflicts in crossJoin\n",
    "control_df = control_df.withColumnRenamed(\"epp_pid\", \"control_pid\") \\\n",
    "    .withColumnRenamed(\"control_score_num\", \"control_score_num\")\n",
    "\n",
    "# Cross join and calculate absolute score difference\n",
    "#matched_df = treated_df.crossJoin(control_df) \\\n",
    "#    .withColumn(\"score_diff\", abs(col(\"propensity_score_num\") - col(\"control_score_num\")))\n",
    "\n",
    "matched_df = control_df.crossJoin(treated_df) \\\n",
    "   .withColumn(\"score_diff\", abs(col(\"control_score_num\") -col(\"propensity_score_num\") ))\n",
    "\n",
    "# Rank by the smallest score difference and keep the best match\n",
    "window_spec = Window.partitionBy(\"control_pid\").orderBy(\"score_diff\")\n",
    "matched_cohort = matched_df.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "                           .filter(col(\"rank\") == 1) \\\n",
    "                           .select(\"epp_pid\", \"control_pid\", \"score_diff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2837a3-28da-4f95-afa7-574b34d673eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(matched_cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eef4669-097b-424d-94e3-a43980bbb549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "matched_epp_ids = matched_cohort.select(\"epp_pid\").rdd.flatMap(lambda x: x).collect()\n",
    "matched_control_ids = matched_cohort.select(\"control_pid\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "waiting_list_df_matched = waiting_list_df.filter(df_weeks_grouped.epp_pid.isin(matched_epp_ids + matched_control_ids))\n",
    "display(waiting_list_df_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc848da8-dc0d-4251-9baf-d2481436968d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    waiting_list_df_matched.groupBy(\n",
    "        \"epp_pid\", \n",
    "        \"epp_tfc\", \n",
    "        \"epp_rtt_start_date\", \n",
    "        \"group\"\n",
    "    ).count().groupBy(\"group\").count()\n",
    ")\n",
    "\n",
    "\n",
    "display(\n",
    "    waiting_list_df.groupBy(\n",
    "        \"epp_pid\", \n",
    "        \"epp_tfc\", \n",
    "        \"epp_rtt_start_date\", \n",
    "        \"group\"\n",
    "    ).count().groupBy(\"group\").count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d3bdc32-edae-4fd6-aeef-198323270e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns2 = [\"ndl_ethnicity\", \"ndl_ltc\", \"Sex\", \"Frailty_level\", \"ndl_imd_quantile\"] \n",
    "#columns2 = [\"ndl_ethnicity\",  \"Sex\",  \"ndl_imd_quantile\"] \n",
    "\n",
    "activity_variable = [\n",
    "    \"gp_healthcare_use\",  \n",
    "    \"ae_healthcare_use\", \"nel_healthcare_use\", #\"el_healthcare_use\", \"op_healthcare_use\", \n",
    "    \"antib_pres_count\", \"antipres_pres_count\", \"pain_pres_count\", \n",
    "]\n",
    "\n",
    "all_estimates = []\n",
    "\n",
    "for activity in activity_variable:\n",
    "\n",
    "    df_30_weeks_filtered = filter_data_function_M(df=waiting_list_df, intervention_group='> 18 weeks', control_group='<= 18 weeks', recovery=28)\n",
    "\n",
    "    df_grouped = group_data_function(\n",
    "        df_filtered=df_30_weeks_filtered,\n",
    "        intervention_group=\"> 18 weeks\",\n",
    "        control_group=\"<= 18 weeks\", \n",
    "        colum_hc=activity,  \n",
    "        col_list=columns2\n",
    "    )\n",
    "    display(df_grouped.groupBy(\"group\").count())\n",
    "\n",
    "    base_formula = f\"avg_weekly_use ~ C(group) * C(time_period)\"\n",
    "\n",
    "# Add interaction terms between waiting time (group) and frailty/LTC\n",
    "    full_formula = base_formula + \" + C(group)*C(Frailty_level) +  C(group)*C(ndl_ltc)+ C(group)*C(ndl_ethnicity)\"\n",
    "\n",
    "# If you have additional variables in `columns2` that you want to add as controls:\n",
    "    #full_formula = full_formula + \" + \" + \" + \".join(columns2)\n",
    "\n",
    "    did_model = smf.ols(formula=full_formula, data=df_grouped.toPandas()).fit()\n",
    "    print(did_model.summary())\n",
    "\n",
    "    estimates = did_model.params\n",
    "    se = did_model.bse\n",
    "\n",
    "    for term in estimates.index:\n",
    "        all_estimates.append({\n",
    "            'Activity': activity,\n",
    "            'Term': term,\n",
    "            'Estimate': estimates[term],\n",
    "            'Standard Error': se[term],\n",
    "            'p-value': did_model.pvalues[term]     \n",
    "         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0710c6d1-5bae-42eb-8446-4da12de65439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_estimates)\n",
    "results_df.to_excel('../Files/OLS_hyster_all.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a933db-1c3d-48f6-8d6c-1dad0f8a3c33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# VIF Calculation Function\n",
    "def calculate_vif(df):\n",
    "    X = add_constant(df)\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Variable'] = X.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Function to perform model diagnostics\n",
    "def perform_diagnostics(model, df):\n",
    "    # Residuals\n",
    "    residuals = model.resid\n",
    "    \n",
    "    # Residual plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.residplot(x=model.fittedvalues, y=residuals, lowess=True, line_kws={'color': 'red'})\n",
    "    plt.title('Residuals vs Fitted')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.show()\n",
    "\n",
    "    # Q-Q plot for Normality of Residuals\n",
    "    qqplot(residuals, line='45')\n",
    "    plt.title('Q-Q Plot')\n",
    "    plt.show()\n",
    "\n",
    "    # Breusch-Pagan Test for Heteroscedasticity\n",
    "    _, pval, _, _ = het_breuschpagan(residuals, model.model.exog)\n",
    "    print(f'Breusch-Pagan test p-value: {pval}')\n",
    "    if pval < 0.05:\n",
    "        print(\"Heteroscedasticity detected (p-value < 0.05)\")\n",
    "    else:\n",
    "        print(\"No heteroscedasticity detected\")\n",
    "\n",
    "    # Cook's Distance for influence/outliers\n",
    "    influence = OLSInfluence(model)\n",
    "    cooks_d = influence.cooks_distance[0]\n",
    "    \n",
    "    # Plot Cook's Distance\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.stem(np.arange(len(cooks_d)), cooks_d, markerfmt=\",\")\n",
    "    plt.title('Cook\\'s Distance')\n",
    "    plt.xlabel('Observation Index')\n",
    "    plt.ylabel('Cook\\'s Distance')\n",
    "    plt.show()\n",
    "\n",
    "    # Check for any high influence points (typically, Cookâ€™s distance > 4/n)\n",
    "    influence_points = np.where(cooks_d > 4 / len(df))[0]\n",
    "    if len(influence_points) > 0:\n",
    "        print(f\"Influential points: {influence_points}\")\n",
    "    else:\n",
    "        print(\"No influential points detected\")\n",
    "\n",
    "# Main loop for each activity\n",
    "all_estimates = []\n",
    "columns2 = [\"ndl_ethnicity\", \"ndl_ltc\", \"Sex\", \"Frailty_level\", \"ndl_imd_quantile\"]\n",
    "\n",
    "activity_variable = [\n",
    "    \"gp_healthcare_use\",  \n",
    "    \"ae_healthcare_use\", \"nel_healthcare_use\", \n",
    "    \"antib_pres_count\", \"antipres_pres_count\", \"pain_pres_count\"\n",
    "]\n",
    "\n",
    "for activity in activity_variable:\n",
    "    df_30_weeks_filtered = filter_data_function_M(df=waiting_list_df_matched, intervention_group='> 18 weeks', control_group='<= 18 weeks', recovery=28)\n",
    "\n",
    "    df_grouped = group_data_function(\n",
    "        df_filtered=df_30_weeks_filtered,\n",
    "        intervention_group=\"> 18 weeks\",\n",
    "        control_group=\"<= 18 weeks\", \n",
    "        colum_hc=activity,  \n",
    "        col_list=columns2\n",
    "    )\n",
    "\n",
    "    \n",
    "    df_grouped_pd = df_grouped.toPandas()\n",
    "\n",
    "  # Build the formula without highly collinear variables\n",
    "    base_formula = f\"avg_weekly_use ~ C(group) * C(time_period)\"\n",
    "    full_formula = base_formula + \" + C(group)*C(Frailty_level) + C(group)*C(ndl_ltc)+ C(group)*C(ndl_ethnicity)\"\n",
    "\n",
    "    did_model = smf.ols(formula=full_formula, data=df_grouped_pd).fit(cov_type='HC3')\n",
    "    \n",
    "    # Model Diagnostics\n",
    "    perform_diagnostics(did_model, df_grouped_pd)\n",
    "\n",
    "    # Filter out influential points based on Cook's distance\n",
    "    influence = OLSInfluence(did_model)\n",
    "    cooks_d = influence.cooks_distance[0]\n",
    "    high_influence = np.where(cooks_d > 4 / len(df_grouped_pd))[0]\n",
    "    df_grouped_pd_filtered = df_grouped_pd.drop(index=high_influence)\n",
    "\n",
    "    did_model_filtered = smf.ols(formula=full_formula, data=df_grouped_pd_filtered).fit(cov_type='HC3')\n",
    "    print(did_model_filtered.summary())\n",
    "\n",
    "    perform_diagnostics(did_model_filtered, df_grouped_pd_filtered)\n",
    "    display(df_grouped_pd_filtered.groupby(\"group\"))\n",
    "\n",
    "\n",
    "\n",
    "    # Store the results\n",
    "    estimates = did_model_filtered.params\n",
    "    se = did_model.bse\n",
    "    for term in estimates.index:\n",
    "        all_estimates.append({\n",
    "            'Activity': activity,\n",
    "            'Term': term,\n",
    "            'Estimate': estimates[term],\n",
    "            'Standard Error': se[term],\n",
    "            'p-value': did_model.pvalues[term]     \n",
    "        })\n",
    "\n",
    "# Convert all estimates into a DataFrame for easy analysis\n",
    "all_estimates_df = pd.DataFrame(all_estimates)\n",
    "\n",
    "# Display the results\n",
    "print(all_estimates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c29f32-7caf-413d-b5ba-096d0341e69f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame(all_estimates)\n",
    "\n",
    "# Plot the estimates with error bars\n",
    "fig, axes = plt.subplots(len(activity_variable), 1, figsize=(10, 5 * len(activity_variable)), sharex=True)\n",
    "\n",
    "for i, activity in enumerate(activity_variable):\n",
    "    activity_df = results_df[results_df['Activity'] == activity]\n",
    "    axes[i].errorbar(activity_df['Estimate'], activity_df['Term'], xerr=activity_df['Standard Error'], fmt='o', markersize=4)\n",
    "    axes[i].axvline(0, color='black', linestyle='--', linewidth=0.8)  # Add vertical line at x=0\n",
    "    axes[i].set_title(f'Estimates for {activity}', fontsize=14)\n",
    "    axes[i].set_xlabel('Estimate', fontsize=14)\n",
    "    axes[i].tick_params(axis='y', labelsize=8)\n",
    "\n",
    "plt.suptitle('Estimates of Healthcare Use with Standard Errors for Different Activities', fontsize=16)\n",
    "plt.subplots_adjust(bottom=0.05, left=0.1, right=0.9, top=0.95, hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5ad9ce-fd96-48be-90ee-a2f4b851d60a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered_results_df = results_df[results_df['Activity'].str.contains('gp')]\n",
    "display(filtered_results_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397dc8b2-a2b3-4b2d-bba8-6b846858ab95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df.to_excel('../Files/OLS_hyster_matched.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ee4176d-930d-4977-a32e-8ac38aed57a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(filtered_results_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Matched_DID_Hysteroscopy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
