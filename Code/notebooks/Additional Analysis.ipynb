{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0abb1c-6c68-40fb-b2ed-41e0baeb1c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "%pip install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d05253c-ef78-45d7-b969-836a0a9cfe88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyspark.sql.functions import mean, stddev, expr, percentile_approx, round\n",
    "from pyspark.sql.functions import datediff, expr, when, col, month, year, avg\n",
    "from pyspark.sql.functions import count, round, sum\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cde2f1c-8801-4790-83da-1b3902b36c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cohort_1_link=\"../Dummy_Data/Cohort_1_synth.xlsx\"\n",
    "cohort_union_df_link = \"../Dummy_Data/Cohort_Synt1.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1e7f013-1813-49ac-bc7d-3c7a6ce95840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cohort_union_df= pd.read_excel(cohort_union_df_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5da18a2-f75f-4751-9404-bd1660d5eb50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_wlmds= pd.read_excel(cohort_1_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47ea3f2-ab59-4d91-9870-e58db0edfdf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tfc_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df_wlmds = df_wlmds.join(tfc_df, df_wlmds.wlmds_treatment_function_code == tfc_df.TFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f32642-06a7-425c-bc24-18fd85125aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter df_wlmds for non-null REG_DATE_OF_DEATH\n",
    "df_wlmds = df_wlmds.filter(\n",
    "    (df_wlmds.epp_rtt_start_date > '2022-03-31') &\n",
    "    (df_wlmds.epp_rtt_end_date < '2024-04-01')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf69f10e-e4fc-4239-b8cc-bfa3cb67aa03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_wlmds = df_wlmds.withColumn(\n",
    "    \"waiting_time_days\",\n",
    "    datediff(df_wlmds.epp_rtt_end_date, df_wlmds.epp_rtt_start_date)\n",
    ").filter(col(\"waiting_time_days\")>0)\n",
    "df_wlmds = df_wlmds.withColumn(\n",
    "    \"waiting_bucket\",\n",
    "    when(df_wlmds.waiting_time_days < 84, \"< 12 weeks\")\n",
    "    .when(df_wlmds.waiting_time_days.between(84, 126), \"12-18 weeks\")\n",
    "    .when(df_wlmds.waiting_time_days.between(127, 245), \"19-36 weeks\")\n",
    "    .when(df_wlmds.waiting_time_days.between(246, 364), \"37-52 weeks\")\n",
    "    .when(df_wlmds.waiting_time_days > 364, \"> 52 weeks\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507a0831-46b4-466c-ad29-41aff846b5ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Time to First Contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b262c8-0e0b-453b-bc99-1aa7870b825c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ttfc = df_wlmds.filter(col(\"epp_source\").startswith(\"wlmds_sus\"))\n",
    "display(ttfc.groupBy(\"epp_pid\", \"epp_tfc\", \"epp_rtt_start_date\").count().count())\n",
    "#display(ttfc)\n",
    "ttfc = ttfc.filter((col(\"epp_rtt_period_sequence\") == 1) & (col(\"epp_sequenced_activity_type\") == 'first_op'))\n",
    "\n",
    "# Calculate days between epp_rtt_start_date and epp_activity_date\n",
    "ttfc = ttfc.withColumn(\"days_between\", datediff(col(\"epp_activity_date\"), col(\"epp_rtt_start_date\")))\n",
    "\n",
    "# Group by Specialty and calculate average and median days_between\n",
    "avg_median_days_per_specialty = ttfc.groupBy(\"Specialty\").agg(\n",
    "    expr(\"avg(days_between) as avg_days_between\"),\n",
    "    expr(\"percentile_approx(days_between, 0.5) as median_days_between\")\n",
    ").orderBy(\"avg_days_between\", ascending=False)\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "pdf_avg_median_days = avg_median_days_per_specialty.toPandas()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(pdf_avg_median_days['Specialty'], pdf_avg_median_days['avg_days_between'], color='skyblue', label='Average Days')\n",
    "plt.bar(pdf_avg_median_days['Specialty'], pdf_avg_median_days['median_days_between'], color='orange', alpha=0.7, label='Median Days')\n",
    "plt.xlabel('Specialty')\n",
    "plt.ylabel('Days Between')\n",
    "plt.title('Average and Median Days Between epp_rtt_start_date and epp_activity_date per Specialty')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1abc9abb-4408-4f83-a6cb-c51428cbb56a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Descriptive Statistics\n",
    "desc_stats = ttfc.groupBy(\"Specialty\").agg(\n",
    "    round(mean(\"days_between\")).alias(\"mean_days_between\"),\n",
    "    round(percentile_approx(\"days_between\", 0.5)).alias(\"median_days_between\"),\n",
    "    round(stddev(\"days_between\")).alias(\"stddev_days_between\"),\n",
    "    round(expr(\"percentile_approx(days_between, 0.75) - percentile_approx(days_between, 0.25)\")).alias(\"iqr_days_between\")\n",
    ")\n",
    "\n",
    "display(desc_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea5f7a6-fadf-4a97-b6f7-5db8e2b916c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(desc_stats.toPandas().to_csv(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a4bef2-20ff-4e6b-a6d2-0424b410d7e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ttfc = df_wlmds.filter(col(\"epp_source\").startswith(\"wlmds_sus\"))\n",
    "display(ttfc.groupBy(\"epp_pid\", \"epp_tfc\", \"epp_rtt_start_date\").count().count())\n",
    "ttfc = ttfc.filter((col(\"epp_rtt_period_sequence\") == 1) & (col(\"epp_sequenced_activity_type\") == 'first_op'))\n",
    "\n",
    "# Calculate days between epp_rtt_start_date and epp_activity_date\n",
    "ttfc = ttfc.withColumn(\"days_between\", datediff(col(\"epp_activity_date\"), col(\"epp_rtt_start_date\")))\n",
    "\n",
    "# Group by Specialty and epp_priority, and calculate average and median days_between\n",
    "avg_median_days_per_specialty_priority = ttfc.groupBy(\"Specialty\", \"epp_referral_priority\").agg(\n",
    "    expr(\"avg(days_between) as avg_days_between\"),\n",
    "    expr(\"percentile_approx(days_between, 0.5) as median_days_between\")\n",
    ").orderBy(\"avg_days_between\", ascending=False)\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "pdf_avg_median_days_priority = avg_median_days_per_specialty_priority.toPandas()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "priorities = pdf_avg_median_days_priority['epp_referral_priority'].unique()\n",
    "for priority in priorities:\n",
    "    subset = pdf_avg_median_days_priority[pdf_avg_median_days_priority['epp_referral_priority'] == priority]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(subset['Specialty'], subset['avg_days_between'], alpha=0.7, label=f'Average Days - Priority {priority}')\n",
    "    plt.bar(subset['Specialty'], subset['median_days_between'], alpha=0.7, label=f'Median Days - Priority {priority}')\n",
    "    plt.xlabel('Specialty')\n",
    "    plt.ylabel('Days Between')\n",
    "    plt.title(f'Average and Median Days Between epp_rtt_start_date and epp_activity_date per Specialty - Priority {priority}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6f204d-5e5d-410a-9a30-fb02b01646cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc0ad27-9a3d-45e1-87e8-70cdd9efb6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_wlmds_death = df_wlmds.filter(\n",
    "    (df_wlmds.REG_DATE_OF_DEATH.isNotNull()) &\n",
    "    (df_wlmds.REG_DATE_OF_DEATH.between(df_wlmds.epp_rtt_start_date, df_wlmds.epp_rtt_end_date))\n",
    ")\n",
    "\n",
    "total=df_wlmds.groupBy(\"epp_pid\",\"epp_tfc\",\"epp_rtt_start_date\", \"epp_rtt_end_date\",  \"Specialty\", \"waiting_bucket\", \"waiting_time_days\", \"epp_referral_priority\").count()\n",
    "death_grouped_df =  df_wlmds_death.groupBy(\"epp_pid\",\"epp_tfc\",\"epp_rtt_start_date\",\"epp_rtt_end_date\", \"Specialty\", \"REG_DATE_OF_DEATH\", \"waiting_bucket\", \"waiting_time_days\", \"epp_referral_priority\").count()\n",
    "\n",
    "\n",
    "death_grouped_df = death_grouped_df.withColumn(\n",
    "    \"cohort_band_conc\",\n",
    "    when((col(\"epp_rtt_start_date\") >= \"2022-01-01\") & (col(\"epp_rtt_start_date\") <= \"2022-03-31\"), \"mar22\")\n",
    "    .when((col(\"epp_rtt_start_date\") >= \"2022-04-01\") & (col(\"epp_rtt_start_date\") <= \"2022-06-30\"), \"jun22\")\n",
    "    .when((col(\"epp_rtt_start_date\") >= \"2022-07-01\") & (col(\"epp_rtt_start_date\") <= \"2022-09-30\"), \"sep22\")\n",
    "    .when((col(\"epp_rtt_start_date\") >= \"2022-10-01\") & (col(\"epp_rtt_start_date\") <= \"2022-12-31\"), \"dec22\")\n",
    "    .when((col(\"epp_rtt_start_date\") >= \"2023-01-01\") & (col(\"epp_rtt_start_date\") <= \"2023-03-31\"), \"mar23\")\n",
    "    .when((col(\"epp_rtt_start_date\") >= \"2023-04-01\") & (col(\"epp_rtt_start_date\") <= \"2023-06-30\"), \"jun23\")\n",
    "    .when((col(\"epp_rtt_start_date\") >= \"2023-07-01\") & (col(\"epp_rtt_start_date\") <= \"2023-09-30\"), \"sep23\")\n",
    "    .when((col(\"epp_rtt_start_date\") >= \"2023-10-01\") & (col(\"epp_rtt_start_date\") <= \"2023-12-31\"), \"dec23\")\n",
    "    .when((col(\"epp_rtt_start_date\") >= \"2024-01-01\") & (col(\"epp_rtt_start_date\") <= \"2024-03-31\"), \"mar24\")\n",
    ")\n",
    "death_grouped_df = death_grouped_df.filter(col(\"cohort_band_conc\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7a8e7e-6983-4096-a239-ab99131c8163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_conc_linked = death_grouped_df.join(\n",
    "    cohort_union_df,\n",
    "    (\n",
    "        (death_grouped_df.cohort_band_conc == cohort_union_df.cohort_band) &\n",
    "        (death_grouped_df.epp_pid == cohort_union_df.Patient_ID)\n",
    "    ),\n",
    "    \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0124c9fa-dbd5-4ca8-b937-2fa6095ea702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_conc_linked.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77040f3-5b8d-456a-b092-7e50cc91cd11",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1742981556248}",
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(death_grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7fbe7d-c601-4626-8f69-e77eff935924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count total records per Specialty\n",
    "total_count_per_specialty = total.groupBy(\"Specialty\").count()\n",
    "\n",
    "# Count records with non-null REG_DATE_OF_DEATH per Specialty\n",
    "death_count_per_specialty = death_grouped_df.groupBy(\"Specialty\").count()\n",
    "\n",
    "# Calculate percentage of death per Specialty\n",
    "percentage_death_per_specialty = death_count_per_specialty.join(\n",
    "    total_count_per_specialty,\n",
    "    on=\"Specialty\"\n",
    ").withColumn(\n",
    "    \"death_percentage\",\n",
    "    round((death_count_per_specialty[\"count\"] / total_count_per_specialty[\"count\"]) * 100, 2)\n",
    ").select(\"Specialty\", \"death_percentage\").orderBy(\"death_percentage\", ascending=False)\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "pdf = percentage_death_per_specialty.toPandas()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(pdf['Specialty'], pdf['death_percentage'], color='skyblue')\n",
    "plt.xlabel('Specialty')\n",
    "plt.ylabel('Death Percentage')\n",
    "plt.title('Death Percentage per Specialty')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Annotate bars with death percentage values\n",
    "for index, value in enumerate(pdf['death_percentage']):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a170a0ee-47ab-441f-8c0c-2dc2aa1b1e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate total death count per specialty\n",
    "total_death_count_per_specialty = death_grouped_df.groupBy(\"Specialty\").count().withColumnRenamed(\"count\", \"total_death_count\")\n",
    "\n",
    "# Calculate death count per waiting bucket and specialty\n",
    "death_count_per_bucket_specialty = death_grouped_df.groupBy(\"waiting_bucket\", \"Specialty\").count().withColumnRenamed(\"count\", \"death_count\")\n",
    "\n",
    "# Join total death count with death count per waiting bucket and specialty\n",
    "percentage_death_per_bucket_specialty = death_count_per_bucket_specialty.join(\n",
    "    total_death_count_per_specialty,\n",
    "    on=\"Specialty\"\n",
    ").withColumn(\n",
    "    \"death_percentage_of_total\",\n",
    "    round((F.col(\"death_count\") / F.col(\"total_death_count\")) * 100, 2)\n",
    ").select(\"waiting_bucket\", \"Specialty\", \"death_percentage_of_total\").orderBy(\"Specialty\", \"waiting_bucket\")\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "pdf = percentage_death_per_bucket_specialty.toPandas()\n",
    "\n",
    "# Pivot and plot\n",
    "pdf_pivot = pdf.pivot(index='Specialty', columns='waiting_bucket', values='death_percentage_of_total')\n",
    "pdf_pivot = pdf_pivot[['< 12 weeks', '12-18 weeks', '19-36 weeks', '37-52 weeks', '> 52 weeks']]\n",
    "pdf_pivot.plot(kind='bar', stacked=True, figsize=(12, 8), colormap='tab20')\n",
    "\n",
    "plt.xlabel('Specialty')\n",
    "plt.ylabel('Death Percentage of Total')\n",
    "#plt.title('Death Percentage of Total per Waiting Bucket for Each Specialty')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.legend(title='Waiting Bucket', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5707244f-7d8e-48af-af2f-8ad0aec305cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculate number of pathways per specialty and referral priority with death during waiting time\n",
    "pathways_with_deaths = death_grouped_df.groupBy(\"Specialty\", \"epp_referral_priority\").count().withColumnRenamed(\"count\", \"pathways_with_deaths\")\n",
    "\n",
    "# Calculate total number of pathways per specialty and referral priority\n",
    "total_pathways = total.groupBy(\"Specialty\", \"epp_referral_priority\").count().withColumnRenamed(\"count\", \"total_pathways\")\n",
    "\n",
    "\n",
    "# Join the two DataFrames\n",
    "result = pathways_with_deaths.join(total_pathways, [\"Specialty\", \"epp_referral_priority\"])\n",
    "\n",
    "# Calculate the percentage of pathways with deaths\n",
    "result = result.withColumn(\n",
    "    'percentage_pathways_with_deaths',\n",
    "    (F.col('pathways_with_deaths') / F.col('total_pathways')) * 100\n",
    ")\n",
    "\n",
    "# Convert to Pandas DataFrame for analysis\n",
    "result_pd = result.orderBy(F.col('percentage_pathways_with_deaths').desc()).toPandas()\n",
    "\n",
    "# Plot the results as grouped bar chart\n",
    "result_pivot = result_pd.pivot(index='Specialty', columns='epp_referral_priority', values='percentage_pathways_with_deaths')\n",
    "result_pivot.plot(kind='bar', figsize=(12, 8))\n",
    "\n",
    "plt.xlabel('Specialty')\n",
    "plt.ylabel('Percentage of Pathways with Deaths')\n",
    "#plt.title('Percentage of Pathways with Deaths by Specialty and Referral Priority')\n",
    "plt.legend(title='Referral Priority')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48b9e60-ef90-4928-aa70-b0e150f75cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate time to death as the difference between registration date and start date\n",
    "death_grouped_df = death_grouped_df.withColumn(\"time_to_death\", datediff(\"REG_DATE_OF_DEATH\", \"epp_rtt_start_date\"))\n",
    "\n",
    "# Perform advanced analysis: summary statistics for time to death\n",
    "summary_stats = death_grouped_df.select(\"time_to_death\").summary()\n",
    "\n",
    "# Display summary statistics\n",
    "display(summary_stats)\n",
    "\n",
    "# Advanced analysis: distribution of time to death by specialty\n",
    "time_to_death_by_specialty = death_grouped_df.groupBy(\"Specialty\").agg(\n",
    "    F.mean(\"time_to_death\").alias(\"mean_time_to_death\"),\n",
    "    F.expr(\"percentile(time_to_death, 0.5)\").alias(\"median_time_to_death\"),\n",
    "    F.stddev(\"time_to_death\").alias(\"stddev_time_to_death\"),\n",
    "    F.min(\"time_to_death\").alias(\"min_time_to_death\"),\n",
    "    F.max(\"time_to_death\").alias(\"max_time_to_death\")\n",
    ").orderBy(\"Specialty\")\n",
    "\n",
    "# Display time to death by specialty\n",
    "display(time_to_death_by_specialty)\n",
    "time_to_death_by_specialty.show()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "time_to_death_pd = time_to_death_by_specialty.toPandas()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x=\"Specialty\", y=\"time_to_death\", data=death_grouped_df.toPandas())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Specialty')\n",
    "plt.ylabel('Time to Death (days)')\n",
    "plt.title('Distribution of Time to Death by Specialty')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9565d5-8bf6-44e1-ac5c-7ff757205171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = death_grouped_df\n",
    "\n",
    "# Step 1: Calculate time to death (in days)\n",
    "df = df.withColumn('time_to_death', datediff(col('REG_DATE_OF_DEATH'), col('epp_rtt_start_date')))\n",
    "df = df.withColumn('death_status', when(col('REG_DATE_OF_DEATH').isNotNull(), 1).otherwise(0))\n",
    "\n",
    "# Ensure 'death_status' column exists\n",
    "if 'death_status' not in df.columns:\n",
    "    raise KeyError(\"Column 'death_status' does not exist in the DataFrame\")\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for further analysis\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "# Step 2: Descriptive statistics of time to death grouped by Specialty and Referral Priority\n",
    "time_to_death_stats = df_pd.groupby(['Specialty', 'epp_referral_priority'])['time_to_death'].describe()\n",
    "print(time_to_death_stats.to_string())\n",
    "\n",
    "# Step 3: Plot distribution of time to death by Specialty and Referral Priority\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='Specialty', y='time_to_death', data=df_pd, hue='epp_referral_priority')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Time to Death by Specialty and Referral Priority')\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Plot distribution of time to death by waiting bucket and Referral Priority\n",
    "waiting_bucket_order = ['< 12 weeks', '12-18 weeks',  '19-36 weeks', '37-52 weeks', '> 52 weeks']\n",
    "df_pd['waiting_bucket'] = pd.Categorical(df_pd['waiting_bucket'], categories=waiting_bucket_order, ordered=True)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='waiting_bucket', y='time_to_death', data=df_pd, hue='epp_referral_priority')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Time to Death by Waiting Bucket and Referral Priority')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Kaplan-Meier survival analysis\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(df_pd['time_to_death'], event_observed=df_pd['death_status'])\n",
    "\n",
    "# Plot survival curve\n",
    "kmf.plot()\n",
    "plt.title('Survival Analysis: Time to Death')\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Statistical test (log-rank test) to compare survival between two groups (e.g., 'urgent' vs. 'routine')\n",
    "group1 = df_pd[df_pd['epp_referral_priority'] == 'urgent']\n",
    "group2 = df_pd[df_pd['epp_referral_priority'] == 'routine']\n",
    "results = logrank_test(group1['time_to_death'], group2['time_to_death'], event_observed_A=group1['death_status'], event_observed_B=group2['death_status'])\n",
    "print(\"Log-rank test results:\", results)\n",
    "\n",
    "# Time to death \n",
    "df_pd = df.select(\"waiting_time_days\", \"death_status\", \"epp_referral_priority\", \"time_to_death\", \"waiting_bucket\").toPandas()\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(df_pd[\"waiting_time_days\"], event_observed=df_pd['death_status'])\n",
    "kmf.plot_survival_function()\n",
    "\n",
    "# Convert data to pandas\n",
    "df_pd = df.select(\"waiting_time_days\", \"death_status\", \"epp_referral_priority\", \"time_to_death\", \"Specialty\", \"waiting_bucket\").toPandas()\n",
    "\n",
    "# Plot survival for each specialty\n",
    "plt.figure(figsize=(12, 8))\n",
    "for specialty in df_pd['waiting_bucket'].unique():\n",
    "    specialty_df = df_pd[df_pd['waiting_bucket'] == specialty]\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(specialty_df[\"waiting_time_days\"], event_observed=specialty_df['death_status'])\n",
    "    kmf.plot_survival_function(label=specialty)\n",
    "\n",
    "plt.xlabel('Waiting Time (Days)')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.legend(title='Specialty')\n",
    "plt.show()\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(df_pd, duration_col='waiting_time_days', event_col='death_status', formula=\"C(waiting_bucket)\") # C() treats waiting_bucket as categorical\n",
    "\n",
    "print(cph.print_summary()) # Prints hazard ratios, confidence intervals, p-values\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"waiting_time_days\"], outputCol=\"features\")\n",
    "df_vector = assembler.transform(df).select(\"features\", \"time_to_death\")\n",
    "\n",
    "correlation = Correlation.corr(df_vector, \"features\", \"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a7ed3f-0b40-460e-8d66-869bfc5af42c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of categories to calculate percentage for\n",
    "categories = [\"AgeBand_5yr\", \"Sex\", \"Ethnic_Group\", \"IMD_Decile\"]\n",
    "\n",
    "# Dictionary to store pivoted tables\n",
    "pivot_tables = {}\n",
    "\n",
    "# Loop through each category and calculate percentages\n",
    "for cat in categories:\n",
    "    # Group and count by waiting_bucket and category\n",
    "    df_conc_linked_g = df_conc_linked.groupBy(cat, \"waiting_bucket\").agg(count(\"*\").alias(\"count\"))\n",
    "    \n",
    "    # Total count per category for percentage calculation\n",
    "    df_total = df_conc_linked_g.groupBy(cat).agg(sum(\"count\").alias(\"total_count\"))\n",
    "\n",
    "    # Join with total count and calculate percentage\n",
    "    df_percentage = df_conc_linked_g.join(\n",
    "        df_total,\n",
    "        cat,\n",
    "        \"left\"\n",
    "    ).withColumn(\n",
    "        \"percentage\",\n",
    "        round((col(\"count\") / col(\"total_count\")) * 100, 2)\n",
    "    )\n",
    "\n",
    "    # Pivot to make waiting_bucket as columns\n",
    "    df_pivot = df_percentage.groupBy(cat).pivot(\"waiting_bucket\").agg(F.first(\"percentage\"))\n",
    "    \n",
    "    # Fill missing values with 0\n",
    "    df_pivot = df_pivot.fillna(0)\n",
    "\n",
    "    # Add pivoted table to dictionary\n",
    "    pivot_tables[cat] = df_pivot\n",
    "\n",
    "\n",
    "\n",
    "display(pivot_tables[\"AgeBand_5yr\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8365d1a-b9a0-4905-b4d2-328ec8dd983f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define waiting buckets order (for consistent ordering in plots)\n",
    "bucket_order = [\"< 12 weeks\", \"12-18 weeks\", \"19-36 weeks\", \"37-52 weeks\", \"> 52 weeks\"]\n",
    "\n",
    "# Plot function for each category\n",
    "def plot_stacked_bar(df, category):\n",
    "    df[bucket_order].plot(\n",
    "        kind='bar',\n",
    "        stacked=True,\n",
    "        figsize=(12, 6),\n",
    "        colormap='viridis'\n",
    "    )\n",
    "    plt.title(f\"Percentage Distribution by {category}\")\n",
    "    plt.xlabel(category)\n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(title=\"Waiting Bucket\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "pivot_dict_pd = {}\n",
    "for cat in pivot_tables:    \n",
    "    pivot_dict_pd[cat] = pivot_tables[cat].toPandas().set_index(cat)\n",
    "\n",
    "# Plot for each category\n",
    "for cat in pivot_dict_pd:\n",
    "    plot_stacked_bar(pivot_dict_pd[cat], cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df5f48a-6e4b-438f-8e80-9d7c83860484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cancelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980dd4c3-5c15-409b-b4ba-46142d851e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_wlmds= spark.read.format(\"parquet\").load(cohort_1_link\")\n",
    "tfc_df = spark.createDataFrame(data, schema=schema)\n",
    "df_wlmds = df_wlmds.join(tfc_df, df_wlmds.wlmds_treatment_function_code == tfc_df.TFC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b447e22a-5420-4e67-bda1-e96b79ea11ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sel_wlmds = df_wlmds.filter(\n",
    "    col(\"epp_source\").contains(\"wlmds_\")\n",
    ").filter(\n",
    "    (col(\"epp_rtt_start_date\") >= '2022-04-01') & (col(\"epp_rtt_end_date\") < '2024-03-31')\n",
    ")\n",
    "display(sel_wlmds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e375000e-3dd9-443f-a278-aa012d001a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Add columns for DNA and Cancellation status\n",
    "sel_wlmds = sel_wlmds.withColumn('is_dna', when(col('sus_attendance_status').contains('dna'), 1).otherwise(0))\n",
    "sel_wlmds = sel_wlmds.withColumn('is_patient_canc', when(col('sus_attendance_status').contains('cancel(pat)'), 1).otherwise(0))\n",
    "sel_wlmds = sel_wlmds.withColumn('is_hospital_canc', when(col('sus_attendance_status').contains('cancel(hos)'), 1).otherwise(0))\n",
    "\n",
    "# Step 2: Group by Specialty and calculate the number of DNAs, Cancellations (both patient and hospital), and unique Pathways\n",
    "result = sel_wlmds.groupBy('Specialty').agg(\n",
    "    # Count DNAs\n",
    "    countDistinct(when(col('is_dna') == 1, col('epp_pathway_id'))).alias('dna_count'),\n",
    "    \n",
    "    # Count Patient Cancellations\n",
    "    countDistinct(when(col('is_patient_canc') == 1, col('epp_pathway_id'))).alias('patient_canc_count'),\n",
    "    \n",
    "    # Count Hospital Cancellations\n",
    "    countDistinct(when(col('is_hospital_canc') == 1, col('epp_pathway_id'))).alias('hospital_canc_count'),\n",
    "    \n",
    "    # Count unique Pathways\n",
    "    countDistinct('epp_pathway_id').alias('unique_pathways')\n",
    ")\n",
    "\n",
    "# Calculate percentages\n",
    "result = result.withColumn('dna_percentage', (col('dna_count') / col('unique_pathways')) * 100)\n",
    "result = result.withColumn('patient_canc_percentage', (col('patient_canc_count') / col('unique_pathways')) * 100)\n",
    "result = result.withColumn('hospital_canc_percentage', (col('hospital_canc_count') / col('unique_pathways')) * 100)\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "pdf_result = result.toPandas()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "pdf_result.plot(kind='barh', x='Specialty', y=['dna_percentage', 'patient_canc_percentage', 'hospital_canc_percentage'], stacked=False)\n",
    "plt.ylabel('Specialty')\n",
    "plt.xlabel('Percentage')\n",
    "#plt.title('DNA and Cancellation Percentages by Specialty')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d11adad-53d6-4095-8396-6e1a3513bb2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract month and year from the attendance date\n",
    "sel_wlmds = sel_wlmds.withColumn('year', year(col('sus_attendance_date_time')))\n",
    "sel_wlmds = sel_wlmds.withColumn('month', month(col('sus_attendance_date_time')))\n",
    "\n",
    "# Group by year, month, and Specialty to get counts of DNAs and cancellations\n",
    "result_by_time = sel_wlmds.groupBy('Specialty', 'year', 'month').agg(\n",
    "    countDistinct(when(col('is_dna') == 1, col('epp_pathway_id'))).alias('dna_count'),\n",
    "    countDistinct(when(col('is_patient_canc') == 1, col('epp_pathway_id'))).alias('patient_canc_count'),\n",
    "    countDistinct(when(col('is_hospital_canc') == 1, col('epp_pathway_id'))).alias('hospital_canc_count')\n",
    ")\n",
    "\n",
    "result_by_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d3a5681-8c1d-4c94-b47a-9e741b05419e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cancellation_rate = sel_wlmds.groupBy('Specialty', 'year', 'month').agg(\n",
    "    countDistinct('epp_pathway_id').alias('total_appointments'),\n",
    "    countDistinct(when(col('is_patient_canc') == 1, col('epp_pathway_id'))).alias('patient_canc_count'),\n",
    "    countDistinct(when(col('is_hospital_canc') == 1, col('epp_pathway_id'))).alias('hospital_canc_count')\n",
    ")\n",
    "\n",
    "# Calculate cancellation rates for patient and hospital cancellations\n",
    "cancellation_rate = cancellation_rate.withColumn(\n",
    "    'patient_canc_rate', (col('patient_canc_count') / col('total_appointments')) * 100\n",
    ").withColumn(\n",
    "    'hospital_canc_rate', (col('hospital_canc_count') / col('total_appointments')) * 100\n",
    ")\n",
    "\n",
    "cancellation_rate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751b8251-b2d9-44bc-a1ee-5f87a9c39552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explore correlation between referral priority and cancellation rates\n",
    "correlation_analysis = sel_wlmds.groupBy('Specialty').agg(\n",
    "    avg('is_dna').alias('avg_dna_rate'),\n",
    "    avg('is_patient_canc').alias('avg_patient_canc_rate'),\n",
    "    avg('is_hospital_canc').alias('avg_hospital_canc_rate')\n",
    ")\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "correlation_analysis_pd = correlation_analysis.toPandas()\n",
    "\n",
    "# Plotting\n",
    "correlation_analysis_pd.plot(x='Specialty', kind='bar', figsize=(12, 8), colormap='tab20')\n",
    "plt.xlabel('Specialty')\n",
    "plt.ylabel('Average Rate')\n",
    "plt.title('Average DNA, Patient Cancellation, and Hospital Cancellation Rates by Specialty')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.legend(title='Rate Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a00a5182-0848-4a21-955e-37ad0d06bfc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add waiting_time column\n",
    "sel_wlmds = sel_wlmds.withColumn(\n",
    "    \"waiting_time\",\n",
    "    datediff(col(\"wlmds_rtt_end_date\"), col(\"wlmds_rtt_start_date\"))\n",
    ")\n",
    "\n",
    "# Filter out rows with waiting_time < 0\n",
    "sel_wlmds = sel_wlmds.filter(col(\"waiting_time\") >= 0)\n",
    "\n",
    "# Define waiting buckets\n",
    "sel_wlmds = sel_wlmds.withColumn(\n",
    "    \"waiting_bucket\",\n",
    "    when((col(\"waiting_time\") <= 18 *7), \"< 18 weeks\")\n",
    "    .when((col(\"waiting_time\") > 18*7) & (col(\"waiting_time\") <= 36*7), \"19-36 weeks\")\n",
    "    .when((col(\"waiting_time\") > 36*7) & (col(\"waiting_time\") <= 52*7), \"37-52 weeks\")\n",
    "    .otherwise(\"> 52 weeks\")\n",
    ")\n",
    "\n",
    "# Group by waiting_bucket and Specialty, and calculate total and cancellation counts\n",
    "total_count_per_bucket_specialty = sel_wlmds.groupBy(\"waiting_bucket\", \"Specialty\").count().withColumnRenamed(\"count\", \"total_count\")\n",
    "hospital_cancellation_count_per_bucket_specialty = sel_wlmds.filter(col(\"is_hospital_canc\") == 1).groupBy(\"waiting_bucket\", \"Specialty\").count().withColumnRenamed(\"count\", \"hospital_cancellation_count\")\n",
    "patient_cancellation_count_per_bucket_specialty = sel_wlmds.filter(col(\"is_patient_canc\") == 1).groupBy(\"waiting_bucket\", \"Specialty\").count().withColumnRenamed(\"count\", \"patient_cancellation_count\")\n",
    "dna_count_per_bucket_specialty = sel_wlmds.filter(col(\"is_dna\") == 1).groupBy(\"waiting_bucket\", \"Specialty\").count().withColumnRenamed(\"count\", \"dna_count\")\n",
    "\n",
    "# Join total, hospital cancellation, patient cancellation, and DNA counts\n",
    "impact_analysis = total_count_per_bucket_specialty.join(\n",
    "    hospital_cancellation_count_per_bucket_specialty,\n",
    "    on=[\"waiting_bucket\", \"Specialty\"],\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    patient_cancellation_count_per_bucket_specialty,\n",
    "    on=[\"waiting_bucket\", \"Specialty\"],\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    dna_count_per_bucket_specialty,\n",
    "    on=[\"waiting_bucket\", \"Specialty\"],\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"hospital_cancellation_percentage\",\n",
    "    round((col(\"hospital_cancellation_count\") / col(\"total_count\")) * 100, 2)\n",
    ").withColumn(\n",
    "    \"patient_cancellation_percentage\",\n",
    "    round((col(\"patient_cancellation_count\") / col(\"total_count\")) * 100, 2)\n",
    ").withColumn(\n",
    "    \"dna_percentage\",\n",
    "    round((col(\"dna_count\") / col(\"total_count\")) * 100, 2)\n",
    ").select(\"waiting_bucket\", \"Specialty\", \"hospital_cancellation_percentage\", \"patient_cancellation_percentage\", \"dna_percentage\").orderBy(\"Specialty\", \"waiting_bucket\")\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "impact_analysis_pd = impact_analysis.toPandas()\n",
    "\n",
    "# Pivot and plot\n",
    "impact_analysis_pivot = impact_analysis_pd.pivot(index='Specialty', columns='waiting_bucket', values=['hospital_cancellation_percentage', 'patient_cancellation_percentage', 'dna_percentage'])\n",
    "impact_analysis_pivot['hospital_cancellation_percentage'].plot(kind='bar', figsize=(12, 8), colormap='tab20', position=0, width=0.25, title='Hospital Cancellation Percentage')\n",
    "impact_analysis_pivot['patient_cancellation_percentage'].plot(kind='bar', figsize=(12, 8), colormap='tab20', position=1, width=0.25, title='Patient Cancellation Percentage')\n",
    "impact_analysis_pivot['dna_percentage'].plot(kind='bar', figsize=(12, 8), colormap='tab20', position=2, width=0.25, title='DNA Percentage')\n",
    "\n",
    "plt.xlabel('Specialty')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Impact Analysis per Waiting Bucket for Each Specialty')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.legend(title='Waiting Bucket', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c715a9c9-1efd-43f8-996d-18e723c14e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate the data to ensure unique combinations of \"Specialty\", \"year\", and \"month\"\n",
    "result_by_time_agg = result_by_time.groupBy(\"Specialty\", \"year\", \"month\").agg(\n",
    "    {\"dna_count\": \"sum\", \"patient_canc_count\": \"sum\", \"hospital_canc_count\": \"sum\"}\n",
    ").withColumnRenamed(\"sum(dna_count)\", \"dna_count\") \\\n",
    " .withColumnRenamed(\"sum(patient_canc_count)\", \"patient_canc_count\") \\\n",
    " .withColumnRenamed(\"sum(hospital_canc_count)\", \"hospital_canc_count\")\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame for plotting\n",
    "result_by_time_pd = result_by_time_agg.toPandas()\n",
    "\n",
    "# Convert columns to integer\n",
    "result_by_time_pd[\"dna_count\"] = result_by_time_pd[\"dna_count\"].astype(int)\n",
    "result_by_time_pd[\"patient_canc_count\"] = result_by_time_pd[\"patient_canc_count\"].astype(int)\n",
    "result_by_time_pd[\"hospital_canc_count\"] = result_by_time_pd[\"hospital_canc_count\"].astype(int)\n",
    "\n",
    "# Create a pivot table for a heatmap\n",
    "pivot_dna = result_by_time_pd.pivot_table(index=\"Specialty\", columns=[\"year\", \"month\"], values=\"dna_count\")\n",
    "pivot_patient_canc = result_by_time_pd.pivot_table(index=\"Specialty\", columns=[\"year\", \"month\"], values=\"patient_canc_count\")\n",
    "pivot_hospital_canc = result_by_time_pd.pivot_table(index=\"Specialty\", columns=[\"year\", \"month\"], values=\"hospital_canc_count\")\n",
    "\n",
    "# Plot heatmaps\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_dna, annot=True, fmt=\".0f\", cmap=\"Blues\", cbar=False)\n",
    "plt.title('DNA Count by Specialty, Year, and Month')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_patient_canc, annot=True, fmt=\".0f\", cmap=\"Oranges\", cbar=False)\n",
    "plt.title('Patient Cancellation Count by Specialty, Year, and Month')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_hospital_canc, annot=True, fmt=\".0f\", cmap=\"Reds\", cbar=False)\n",
    "plt.title('Hospital Cancellation Count by Specialty, Year, and Month')\n",
    "plt.show()\n",
    "\n",
    "# Print pivot tables as string\n",
    "print(pivot_dna.to_string())\n",
    "print(pivot_patient_canc.to_string())\n",
    "print(pivot_hospital_canc.to_string())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Additional Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
